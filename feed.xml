<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://blog.joshgav.com//feed.xml" rel="self" type="application/atom+xml" /><link href="https://blog.joshgav.com//" rel="alternate" type="text/html" hreflang="en" /><updated>2023-05-08T16:04:12+00:00</updated><id>https://blog.joshgav.com//feed.xml</id><title type="html">Partly Cloudy</title><subtitle>A blog about cloud infrastructure and cloud-native application patterns and practices.</subtitle><author><name>Josh Gavant</name><email>joshgavant@gmail.com</email></author><entry><title type="html">Kubecon Amsterdam Learnings</title><link href="https://blog.joshgav.com//posts/kubecon-amsterdam" rel="alternate" type="text/html" title="Kubecon Amsterdam Learnings" /><published>2023-05-08T07:00:00+00:00</published><updated>2023-05-08T07:00:00+00:00</updated><id>https://blog.joshgav.com//posts/kubecon-amsterdam</id><content type="html" xml:base="https://blog.joshgav.com//posts/kubecon-amsterdam"><![CDATA[<p><img width="600" alt="Amsterdam" src="../assets/amsterdam.jpg" /></p>

<p>Kubecon EU 2023 in Amsterdam was a great opportunity to discuss and learn more about emerging trends in cloud-native architecture and engineering. My group <a href="https://tag-app-delivery.cncf.io/">TAG App Delivery</a> maintained a booth in the project pavilion and ran <a href="https://tag-app-delivery.cncf.io/blog/tag-app-delivery-at-kubecon-eu-2023/">a lightning talk series</a>) where we met and introduced many project maintainers enabling app delivery and development on Kubernetes and open cloud-native platforms.</p>

<p>It’s already been two weeks since then, so before it gets away from me I wanted to share notes and thoughts on the trends I heard a lot about. Here goes!</p>

<h2 id="contents">Contents</h2>

<ul>
  <li><a href="#portals-and-platforms-work-together">Portals and platforms work together</a></li>
  <li><a href="#two-levels-for-kubernetes-apis">Two levels for Kubernetes APIs</a></li>
  <li><a href="#oci-is-about-more-than-container-images">OCI is about more than container images</a></li>
  <li><a href="#measure-twice">Measure twice!</a></li>
  <li><a href="#burgeoning-cncf-ecosystem">Burgeoning CNCF ecosystem</a></li>
  <li><a href="#maximizing-collaboration">Maximizing collaboration</a></li>
</ul>

<h2 id="portals-and-platforms-work-together">Portals and platforms work together</h2>

<p>As chair of <a href="https://tag-app-delivery.cncf.io/about/wg-platforms/">CNCF’s WG Platforms</a> and lead contributor on our recent <a href="https://tag-app-delivery.cncf.io/whitepapers/platforms/">Platforms white paper</a> I’ve been talking with folks about the rise of Backstage a lot. I’ve learned that to some folks Backstage or similar projects <em>are</em> a platform; but per the working group’s consensus these are interfaces - portals - enabling better user experiences. They are <em>part of</em> a platform, <em>part of</em> its interface.</p>

<p>At Kubecon I talked with <a href="https://www.linkedin.com/in/jrlainfiesta/">Jorge Lainfiesta</a> of <a href="https://roadie.io/">Roadie</a>, <a href="https://www.linkedin.com/in/suzannedaniels/">Suzanne Daniels</a> of <a href="https://www.getport.io/">Port</a> and <a href="https://www.linkedin.com/in/rajith-muditha-attapattu/">Rajith Attapattu</a> of <a href="https://www.randoli.ca/">AppDirector</a>; they and others all seem to agree that our customers asking for portals really want a portal <em>with</em> a backing platform, but the backing platform is an implementation detail of the full experience represented by the portal. A complete platform typically includes an excellent portal to enable users; but a portal <em>without</em> a platform isn’t much more than a fancy manual.</p>

<p>At Red Hat we’re also developing a portal based on Backstage known as <a href="https://janus-idp.io/https://github.com/janus-idp">Janus</a> to complement our OpenShift and other platforms. My Kubecon conversations have me encouraging Red Hat to thoughtfully decouple our “portal” and “platform” efforts. That is, I believe the same developer portal framework can be applicable to OpenShift, to other Kubernetes distributions, or even to native cloud platforms like AWS and Azure. And the same platform might offer one of any number of portals like Backstage or <a href="https://ortelius.io/">Ortelius</a>.</p>

<h2 id="two-levels-for-kubernetes-apis">Two levels for Kubernetes APIs</h2>

<p>In addition to a graphical portal, the other major interface to a platform is its APIs.</p>

<p>I’ve been an advocate of using the Kubernetes API framework for all software-defined infrastructure for a while (see <a href="https://blog.joshgav.com/posts/kubernetes-isnt-about-containers">this post</a>). But perhaps <a href="https://twitter.com/justinsantab">Justin Santa Barbara</a>’s short presentation at the TAG App Delivery booth on Wednesday helped crystallize the crux of this position - that Kubernetes APIs are the <em>point of interoperability</em> for software-defined infrastructure.</p>

<p>That is, while Kubernetes APIs are the de facto open standard, they are <em>not</em> and don’t need to be the APIs exposed to platform <em>users</em> and application developers. These users want a reduction of complete Kubernetes APIs, they want small resource representations that “compile to” all the typical (Kubernetes) components required by microservices - service, deployment, certificate, build, image repository, monitoring, etc.</p>

<p>To meet this demand a plethora of projects exist and continue to emerge to reduce Kubernetes APIs to smaller resource types fit for specific use cases. In my opinion <a href="https://helm.sh/">Helm</a> and it’s values.yaml “APIs” is an example of this pattern; more modern possibilities include <a href="https://kratix.io/">Kratix</a>’s <a href="https://kratix.io/docs/main/reference/promises/intro">promises</a> and <a href="https://www.crossplane.io/">Crossplane</a>’s <a href="https://docs.crossplane.io/v1.12/concepts/composition/">Composite Resources</a>.</p>

<p>This distinction between full Kubernetes APIs (aka custom resource definitions) and API “views” for platform users leads me to continue to advocate for Kubernetes resources as a “source of truth” and point of interobility and policy for software-defined infrastructure, while in parallel seeking conventions for the API-reducing frameworks seeking to encapsulate the complexity of those full APIs.</p>

<h2 id="oci-is-about-more-than-container-images">OCI is about more than container images</h2>

<p>Another technical trend in CNCF projects is the use of OCI packages for more than container images. TAG App Delivery is <a href="https://github.com/cncf/tag-app-delivery/issues/368">forming WG Artifacts</a> to seek synergies in this space.</p>

<p>The <a href="https://opencontainers.org/">Open Container Initiative (OCI)</a> has standardized container image bundles so that they can be extracted and deployed by any container runtime like <a href="https://cri-o.io/">CRIO</a> or <a href="https://containerd.io/">containerd</a>. Building on that success OCI has defined a format for other artifact types to be packaged in the same way -  <a href="https://github.com/opencontainers/artifacts">OCI Artifacts</a>. Many projects are adopting this format to bundle more than container images for deployment in Kubernetes clusters.</p>

<p>For example, projects including <a href="https://www.acorn.io/">Acorn</a>, <a href="https://carvel.dev/imgpkg/">Carvel imgpkg</a>, <a href="https://helm.sh/docs/topics/registries/">Helm</a>, <a href="https://www.openpolicyagent.org/docs/latest/management-bundles/">Open Policy Agent</a> and <a href="https://olm.operatorframework.io/docs/tasks/creating-operator-bundle/">Operator Framework</a> all bundle declarative manifests and other binary formats into OCI packages and distribute these via OCI registries like <a href="https://zotregistry.io/">zot</a>, <a href="https://goharbor.io/">Harbor</a> and <a href="https://artifacthub.io/">ArtifactHub</a>. <strong>Like CRIO and containerd in Linux for container images, controllers in Kubernetes unbundle and deploy the contents of these OCI bundles.</strong></p>

<p>The proliferation of OCI-based bundle formats suggests great opportunity for reducing complexity in software development - imagine a day like my friend <a href="https://www.linkedin.com/in/stevelasker/">Steve Lasker</a> does <a href="https://stevelasker.blog/2021/08/26/artifact-services/">here</a> that all software artifacts and dependencies are bundled in the same package types and searched and verified in a standard way.</p>

<p>WG Artifacts is <a href="https://github.com/cncf/tag-app-delivery/issues/368">being started</a> in TAG App Delivery to bring together these various projects. An early goal for the group is to enable consistent search for metadata about these many artifact types, such as verifying attestations and SBOMs.</p>

<h2 id="measure-twice">Measure twice!</h2>

<p>The goals of cloud-native observability have expanded beyond optimizing for performance and user experience. We’ve realized we should measure other observable aspects of our systems too like costs and energy or carbon consumption. Projects like <a href="https://sustainable-computing.io/">Kepler</a> and <a href="https://www.opencost.io/">OpenCost</a> and committees like <a href="https://github.com/cncf/tag-env-sustainability">TAG Environmental Sustainability</a> are emerging to rationalize and speed adoption of these projects.</p>

<p>These new observable aspects of our systems will be gradually integrated into existing suites so that developers and product managers can monitor costs and environmental impact in the same dashboards as performance and user experience. As a corollary, a theme I noted in observability discussions is <em>correlation</em>, i.e., the ability to gather related observations for analysis, such as the energy consumption of a specific subsystem at the time of an error.</p>

<h2 id="burgeoning-cncf-ecosystem">Burgeoning CNCF ecosystem</h2>

<p>The proliferation of projects in CNCF and the need to understand and support them was a theme of keynotes from CNCF leaders. I was inspired by ideas for encouraging new contributors from <a href="https://www.youtube.com/watch?v=rOsrfxjhev0">Dawn Foster</a> and <a href="https://www.youtube.com/watch?v=oKHD3yAyWss">Emily Fox</a> and wrote up a new <a href="https://github.com/cncf/tag-app-delivery/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a> doc for TAG App Delivery based on discussions at our booth.</p>

<p>I personally lead TAG App Delivery cause I belive cloud-native projects will better succeed in enabling end users when these projects and users are aware of each other and adopt similar patterns and practices. We don’t want to slow down individual projects and innovations, but we do want to facilitate cross-pollination, ultimately to reduce complexity and ensure adoption and usability of projects by end users.</p>

<p>Stated from another perspective, we must recognize and appreciate the effect of <a href="https://martinfowler.com/bliki/ConwaysLaw.html">Conway’s Law</a> on CNCF’s ecosystem and take some steps to reduce its impact.</p>

<p>A great example of this potential synergy is the composition of <a href="https://www.crossplane.io/">Crossplane</a> resources, <a href="https://knative.dev/">Knative</a> services, and <a href="https://dapr.io/">Dapr</a> capabilities as described in <a href="https://www.linkedin.com/in/salaboy/">Mauricio Salatino</a>’s posts like <a href="https://www.salaboy.com/2022/12/24/dapr-and-knative-functions/">this one</a>. That synergy crystallized for many of us (IMHO!) in discussions at Kubecon Detroit in October 2022.</p>

<p>The discussion of project growth also reminded me of one of the values I find in participating in CNCF - exposure to lots of new ideas and projects like <a href="https://github.com/orgs/cncf/projects/14">those pending for sandbox</a>.  At Kubecon alone I learned more about projects <a href="https://github.com/cncf/sandbox/issues/38">K8sGPT</a>, a tool for diagnosing cluster problems; <a href="https://github.com/cncf/sandbox/issues/37">Microcks</a>, a tool for mocking microservice APIs; <a href="https://www.paralus.io/">Paralus</a>, a multicluster authorization gateway; and <a href="https://github.com/cncf/sandbox/issues/34">kpt</a>, a compiler of sorts for reduced “Kubernetes” APIs. And I also learned about future sandbox projects <a href="https://gimlet.io/">gimlet.io</a> and <a href="https://clastix.io/">clastix</a>, to name just a few!</p>

<h2 id="maximizing-collaboration">Maximizing collaboration</h2>

<p>I often say the COVID-19 pandemic helped me appreciate the value of interpersonal relationships and in-person meetups. I’m quite introverted yet the difficulty in sharing ideas and projects during that time led me to recognize the importance of conversations, meetings and informal sharing in order to develop shared understandings and progress together.</p>

<p>In TAG App Delivery, ongoing conversations in <a href="https://slack.cncf.io/">CNCF Slack</a> together with two monthly video meetings and two yearly in-person meetings help us develop shared ideas and hopefully reduce complexity for project maintainers and users.</p>

<p>With so many technical and project/product leaders together at our two annual “Cloud Native” conferences I hope we’ll keep working on maximizing this time together. For example, might we bring TAG leaders and members together at the CNCF project pavilion or in meetups? Might we refine pre-day and auxiliary events so that groups can meet and learn from each other? Can we better enable related projects to find each other and discover opportunities for collaboration?</p>

<p>I hope to pursue these opportunities with other CNCF leads and implement some at <a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/">Kubecon Chicago</a> - hope to see you there!</p>]]></content><author><name>Josh Gavant</name><email>joshgavant@gmail.com</email></author><category term="kubernetes" /><category term="portals" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Platforms at Kubecon 2022</title><link href="https://blog.joshgav.com//posts/kubecon-platforms-review" rel="alternate" type="text/html" title="Platforms at Kubecon 2022" /><published>2022-10-31T07:00:00+00:00</published><updated>2022-10-31T07:00:00+00:00</updated><id>https://blog.joshgav.com//posts/kubecon-platforms-review</id><content type="html" xml:base="https://blog.joshgav.com//posts/kubecon-platforms-review"><![CDATA[<p>Kubecon on October 25 was a great chance to meet people and discuss emerging ideas for making cloud computing easier and more efficient for practitioners and enterprises. Cloud <em>platforms</em> and <em>platform engineering</em> were a popular discussion item and seem poised to finally offer a path for enterprises to maximize dev/ops cooperation and efficiency. In the interest of better understanding the requirements of such platforms for my role as lead of CNCF’s Platforms working group (WG), I spent time talking to leaders and project maintainers developing building blocks for them. The WG plans to <a href="https://github.com/cncf/tag-app-delivery/issues/246">provide a guide</a> on capabilities and requirements of typical platforms by the end of 2022.</p>

<p>In particular, the Platforms WG held an “unmeetup” (slides <a href="https://docs.google.com/presentation/d/1LoAzgZe3rJuxHT86Cdj6Tv4-0XpL7nN6SMCl6oN4JbU/">here</a>) where several platform component providers introduced themselves and shared a brief technical introduction to their projects to facilitate awareness and cooperation. <a href="https://crossplane.io/">Crossplane</a>, <a href="https://score.dev/">Score</a>, <a href="https://dapr.io/">Dapr</a>, <a href="https://github.com/operator-framework/rukpak">Rukpak</a>, <a href="https://www.vcluster.com/">VCluster</a>, <a href="https://github.com/syntasso/kratix">Kratix</a>, <a href="https://kcp.io/">kcp</a> and <a href="https://servicebinding.io/">servicebinding.io</a> were all represented and discussed as we seek to categorize functionality and increase compatibility to ultimately reduce complexity for platform engineers and help them succeed.</p>

<p>I shared in our meetup an early attempt at categorizing some of the required functions of a cloud platform, come discuss it in <a href="https://cloud-native.slack.com/archives/C020RHD43BP">our Slack channel</a>:</p>

<p><img src="../assets/platform_components.png" alt="Platform components" /></p>

<h2 id="notable-trends">Notable trends</h2>

<p>Following are some trends I noted at Kubecon relevant to platform builders:</p>

<ol>
  <li>
    <p><strong>Projects are emerging to aid in composition and abstraction of collections of
Kubernetes resources.</strong> Most platform engineers are happy using Kubernetes APIs
via YAML manifests as the foundation for provisioning and managing services and
capabilities from their platform; but most desire to provide a reduced
abstraction over these resources for their product developers, exposing only
those parameters relevant in their environments. While Helm and its values.yaml
files have provided such a paradigm for a while, several more advanced projects have
emerged that allow platform engineers to curate reduced APIs: Crossplane’s
<a href="https://crossplane.io/docs/v1.9/concepts/composition.html">Composite Resources</a>, Kratix’s <a href="https://kratix.io/docs/workshop/installing-a-promise">Promises</a>, and Google’s <a href="https://kpt.dev/book/02-concepts/01-packages">kpt</a> are examples.</p>

    <p>It would be helpful to compare these composition patterns as well as related
models from Terraform, AWS Cloud Formation and Azure Resource Manager and
iterate forward together.</p>
  </li>
  <li>
    <p><strong>Service catalogs are emerging to gather and advertise curated services.</strong>
Modern digital products and services depend on many capabilities and services
from a platform, from block and object storage to databases, identities and
monitoring systems. These digital products may also use other line-of-business
services.  Projects like
<a href="https://docs.kcp.io/kcp/main/concepts/quickstart-tenancy-and-apis/#publish-some-apis-as-a-service-provider">kcp</a>
and <a href="https://github.com/kube-bind/kube-bind">kubectl-bind</a> make it possible for
platform engineers to offer self-managed and provider-managed services via
Kubernetes APIs just like pods, volumes and network gateways. Internal service
catalogs like <a href="https://ortelius.io/">Ortellius</a> can collate line-of-business
services. And portals like <a href="https://backstage.io/">Backstage</a> can be used to
make all these services easy to find, provision and observe on demand.
Developers can choose services they require from catalogs and create
reproducible environments, perhaps even in a virtual cluster like
<a href="https://www.vcluster.com/">vcluster</a>.</p>
  </li>
  <li>
    <p><strong>Service binding patterns are proliferating.</strong> As it gets easier to provision
additional services and capabilities via Kubernetes, users need ways to retrieve
“binding” info once a service is ready. Service “bindings” could include a URL
for dataplane access, credentials for authorization and endpoints for logs and
metrics. Hashicorp’s <a href="https://developer.hashicorp.com/vault/docs/agent/template">Vault Agent</a> and its <a href="https://developer.hashicorp.com/vault/docs/secrets">secrets engines</a> are one way to get such
bindings, but other mechanisms also exist like Crossplane’s <a href="https://github.com/crossplane/crossplane/blob/master/design/design-doc-external-secret-stores.md#api">secret stores</a> and
<a href="https://servicebinding.io/">servicebinding.io</a>. Many tools write connection details to a resource’s status or
a named Kubernetes Secret or ConfigMap and require users to read the docs and
learn each tool’s conventions.</p>

    <p>It would be helpful to converge towards a standard set of bindings generators
and standard locations to put this information in resource descriptors and
runtime environments.</p>
  </li>
  <li>
    <p><strong>Bundlers and deployers based on OCI are popular.</strong> Several projects now offer
to bundle container images alongside infrastructure descriptors in OCI (Open
Container Initiative) packages; these packages are then retrieved, unbundled and
applied by custom controllers running in Kubernetes clusters. Examples include
<a href="https://porter.sh/architecture/">porter</a>, <a href="https://docs.acorn.io/publishing">acorn</a>, Carvel’s <a href="https://carvel.dev/imgpkg/">imgpkg</a> and Operator Lifecycle Manager <a href="https://github.com/operator-framework/operator-registry/blob/master/docs/design/operator-bundle.md">bundles</a>. Valuably,
the <a href="https://github.com/operator-framework/rukpak">rukpak</a> project aims to offer a package manager which supports many different
bundle formats via <a href="https://github.com/operator-framework/rukpak/blob/main/docs/provisioners/overview.md">Provisioners</a>.</p>

    <p>It would be great to see unbundlers and provisioners for several different
formats in one controller like Rukpak.</p>
  </li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>Cloud platforms and platform teams promise to make cloud-native application development more efficient; and lots of tools and patterns are emerging to make platform builders’ jobs easier.</p>

<p>Our next work in CNCF’s Platforms WG aims to support these builders by defining key components of a cloud-native platform. We’ll also bring together providers of similar component types and seek conventions and patterns to increase compatibility and make usage easier for platform engineers. Join us!</p>]]></content><author><name>Josh Gavant</name><email>joshgavant@gmail.com</email></author><category term="kubernetes" /><category term="platforms" /><summary type="html"><![CDATA[Kubecon on October 25 was a great chance to meet people and discuss emerging ideas for making cloud computing easier and more efficient for practitioners and enterprises. Cloud platforms and platform engineering were a popular discussion item and seem poised to finally offer a path for enterprises to maximize dev/ops cooperation and efficiency. In the interest of better understanding the requirements of such platforms for my role as lead of CNCF’s Platforms working group (WG), I spent time talking to leaders and project maintainers developing building blocks for them. The WG plans to provide a guide on capabilities and requirements of typical platforms by the end of 2022.]]></summary></entry><entry><title type="html">Run OpenShift on AWS</title><link href="https://blog.joshgav.com//posts/openshift-on-aws" rel="alternate" type="text/html" title="Run OpenShift on AWS" /><published>2022-08-09T07:00:00+00:00</published><updated>2022-08-09T07:00:00+00:00</updated><id>https://blog.joshgav.com//posts/openshift-on-aws</id><content type="html" xml:base="https://blog.joshgav.com//posts/openshift-on-aws"><![CDATA[<p><strong>Summary</strong>: Descriptions and simple scripts for supported ways to deploy OpenShift on Amazon Web Services - ROSA, IPI and UPI.</p>

<p><img src="../assets/openshift_on_aws/cloud-rose.jpg" width="400px" style="float: right; margin: 5px" alt="a rosy cloud; credit: https://www.pinterest.com/pin/782993085208691059/" /></p>

<h2 id="overview">Overview</h2>

<p>OpenShift is Kubernetes with batteries included and verified - that is,
OpenShift is ready to run complex first- and third-party applications and
digital workloads as soon as installation is complete. For example, every
OpenShift cluster includes a software-defined network provider, a container and
source build system and artifact registry, an Internet-facing router and even a
system for maintaining and updating cluster components. Contrast this with a
cluster provisioned by upstream
<a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/">kubeadm</a>
or even the more featureful
<a href="https://kubernetes.io/docs/setup/production-environment/tools/kubespray/">kubespray</a>,
where adding these and other critical features requires thoughtful and sometimes
fragile integration.</p>

<p>Not only does OpenShift offer a ready-to-use Kubernetes environment for
developers and operators once deployed, but deployment itself has also become
remarkably simple despite the many parts, especially when a cluster is deployed
in a public cloud provider like AWS. Gone are the days when operators and
infrastructure teams would have to pre-provision hardware and operating systems
and carefully configure and patch datacenter networks before cluster
installation could even begin. Though it’s still possible to customize OpenShift
and its supporting infrastructure to your heart’s content, many use cases can be
met with just the defaults, a cloud provider account and a few well-written
commands.</p>

<p>In this article we’ll discuss three ways to deploy and run OpenShift on Amazon
Web Services (AWS), and we’ll contrast these with deploying upstream Kubernetes
with kubespray. The methods are presented in order of increasing complexity and
customizability: ROSA -&gt; IPI -&gt; UPI -&gt; Kubespray.</p>

<p>Follow along and contribute to code at
<a href="https://github.com/joshgav/openshift-on-aws.git">https://github.com/joshgav/openshift-on-aws.git</a>.</p>

<h2 id="contents">Contents</h2>

<ol>
  <li><a href="#rosa">Red Hat OpenShift Service on AWS (ROSA)</a></li>
  <li><a href="#ipi">OpenShift Installer-provisioned infrastructure</a></li>
  <li><a href="#upi">OpenShift User-provisioned infrastructure</a></li>
  <li><a href="#kubespray">Kubernetes with Kubespray</a></li>
</ol>

<p><a id="rosa"></a></p>

<h2 id="rosa-red-hat-openshift-service-on-aws">ROSA: Red Hat OpenShift Service on AWS</h2>

<blockquote>
  <p><a href="https://github.com/joshgav/openshift-on-aws/tree/main/rosa">Follow along with scripts</a>.</p>
</blockquote>

<p>Let’s start with the simplest option: Red Hat OpenShift Service on AWS, “ROSA”.
A ROSA cluster includes deployment, configuration <em>and</em> management of required
compute, network and storage resources in AWS as well as all the resources and
services of a OpenShift Kubernetes cluster. As opposed to other options to
follow, a ROSA environment is fully supported by Red Hat’s operations teams -
open a ticket and an expert Red Hat SRE will attend to it quickly.</p>

<h3 id="setup">Setup</h3>

<p>At the core of the ROSA lifecycle are the <a href="https://docs.openshift.com/rosa/rosa_cli/rosa-get-started-cli.html">rosa
CLI</a> tool
and the <a href="https://docs.openshift.com/rosa/ocm/ocm-overview.html">OpenShift Cluster
Manager</a> (OCM) service
(UI: <a href="https://console.redhat.com/openshift">https://console.redhat.com/openshift</a>).  Get the CLI from the
<a href="https://console.redhat.com/openshift/downloads">Downloads</a> section of the Red
Hat Console (free account required) or directly from
<a href="https://mirror.openshift.com/pub/openshift-v4/clients/rosa/latest/rosa-linux.tar.gz">https://mirror.openshift.com/pub/openshift-v4/clients/rosa/latest/rosa-linux.tar.gz</a>.
Source is at <a href="https://github.com/openshift/rosa">https://github.com/openshift/rosa</a>. The ROSA CLI invokes OCM
services as described at <a href="https://api.openshift.com/">https://api.openshift.com/</a>, which in turn provision
required infrastructure.</p>

<p>You’ll need both Red Hat and AWS credentials to enable the <code class="language-plaintext highlighter-rouge">rosa</code> CLI to
provision and connect to resources. Your AWS credentials can be specified as
exported <code class="language-plaintext highlighter-rouge">AWS_ACCESS_KEY_ID</code>, <code class="language-plaintext highlighter-rouge">AWS_SECRET_ACCESS_KEY</code> and <code class="language-plaintext highlighter-rouge">AWS_REGION</code>
environment variables as
<a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html">for the AWS CLI</a>.</p>

<p>To get a token to login to your Red Hat account, click “View API token” at the
bottom of the Downloads page as shown in the following screenshots, or go
straight to <a href="https://console.redhat.com/openshift/token">the token page</a>. On
that page click “Load token”, then copy the raw token (not the <code class="language-plaintext highlighter-rouge">ocm</code> command
line) and run <code class="language-plaintext highlighter-rouge">rosa login --token="${your_token}"</code>. If successful you will see
this message (with your username of course): <code class="language-plaintext highlighter-rouge">"I: Logged in as 'joshgavant' on
'https://api.openshift.com'</code>.</p>

<p><img src="../assets/openshift_on_aws/console-downloads-token.png" width="300px" />
<img src="../assets/openshift_on_aws/ocm-manage-token.png" width="300px" /></p>

<p>To verify that you’ve logged in successfully to both accounts run <code class="language-plaintext highlighter-rouge">rosa whoami</code>.
If connections were successful you’ll see a list of attributes about each
account.</p>

<blockquote>
  <p><strong>Tip</strong>: To quickly enable autocompletion for <code class="language-plaintext highlighter-rouge">rosa</code> commands in your current
  shell session run <code class="language-plaintext highlighter-rouge">. &lt;(rosa completion)</code>.</p>
</blockquote>

<h3 id="iam-roles">IAM Roles</h3>

<p>Next you’ll need to create and link AWS IAM roles defining the limited
permissions granted to the cluster manager service and Red Hat operations team
members. In the recommended “STS” mode, these roles are applied to short-lived
tokens issued to machine and human operators on demand.</p>

<p>The following commands grant required access to the OpenShift Cluster Manager
(OCM) and its installers. The last command creates roles to act as profiles for
the EC2 instances. By specifying <code class="language-plaintext highlighter-rouge">--admin</code> and <code class="language-plaintext highlighter-rouge">--mode=auto</code> several additional
roles will be automatically created during installation.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rosa create <span class="nt">--yes</span> ocm-role <span class="nt">--admin</span> <span class="nt">--mode</span><span class="o">=</span>auto <span class="nt">--prefix</span><span class="o">=</span><span class="s2">"ManagedOpenShift"</span>
rosa create <span class="nt">--yes</span> user-role <span class="nt">--mode</span><span class="o">=</span>auto <span class="nt">--prefix</span><span class="o">=</span><span class="s2">"ManagedOpenShift"</span>
rosa create <span class="nt">--yes</span> account-roles <span class="nt">--mode</span><span class="o">=</span>auto <span class="nt">--prefix</span><span class="o">=</span><span class="s2">"ManagedOpenShift"</span>
</code></pre></div></div>

<h3 id="create-cluster">Create cluster</h3>

<p>Now that your Red Hat account is bound to your AWS account you can proceed to
create your ROSA cluster! Here we’ll continue to use the rosa CLI; later we’ll
mention another approach. Run the following command to create a cluster in <a href="https://docs.openshift.com/rosa/rosa_getting_started/rosa-sts-getting-started-workflow.html">STS
mode</a>.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">CLUSTER_NAME</span><span class="o">=</span>rosa1
rosa create <span class="nt">--yes</span> cluster <span class="nt">--cluster-name</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--sts</span> <span class="nt">--mode</span><span class="o">=</span>auto <span class="nt">--watch</span>
</code></pre></div></div>

<p>You can also interactively provide configuration options at cluster creation
time by running <code class="language-plaintext highlighter-rouge">rosa create cluster</code> and answering the prompts.</p>

<h3 id="monitor-installation">Monitor installation</h3>

<p>By setting the <code class="language-plaintext highlighter-rouge">--watch</code> flag in the above command installation logs will stream
to stdout and the command won’t return till installation completes successfully
or fails, typically &gt;30 minutes. You can also start watching logs anytime with
<code class="language-plaintext highlighter-rouge">rosa logs install --cluster ${CLUSTER_NAME} --watch</code>.</p>

<p>Finally, you can review logs and other attributes of your new cluster in the
<a href="https://console.redhat.com/openshift">Red Hat Console</a>. Click into it and
expand the “Show logs” section to reach a view like the following:</p>

<p><img src="../assets/openshift_on_aws/view-cluster-webui.png" width="300px" /></p>

<h3 id="use-cluster">Use cluster</h3>

<p>Once ready, the easiest way to begin using your cluster immediately is to
create a one-off <code class="language-plaintext highlighter-rouge">cluster-admin</code> user as follows. Later you can allow users from
a specific OpenIDConnect (OIDC) identity provider using <code class="language-plaintext highlighter-rouge">rosa create
oidc-provider ...</code>.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">## create a cluster-admin user</span>
rosa create <span class="nt">--yes</span> admin <span class="nt">--cluster</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">"</span>
</code></pre></div></div>

<p>You’ll need URLs to reach the API server and web console of your new cluster;
get those with <code class="language-plaintext highlighter-rouge">rosa list clusters</code>. Finally, log in to the cluster via the <code class="language-plaintext highlighter-rouge">oc</code>
CLI: <code class="language-plaintext highlighter-rouge">oc login --user cluster-admin --password ${admin_password}</code>.</p>

<h3 id="create-cluster-via-ui">Create cluster via UI</h3>

<p>Instead of using the <code class="language-plaintext highlighter-rouge">rosa</code> CLI, once your Red Hat and AWS accounts have been
linked as described above you can also choose to create a cluster via a guided
graphical wizard in the Console. On the <a href="https://console.redhat.com/openshift">Clusters page</a>
on the Red Hat Console click “Create cluster”, then on the <a href="https://console.redhat.com/openshift/create">Cluster create
page</a> click “Create cluster” next
to the ROSA offering, as in the following screenshot:</p>

<p><img src="../assets/openshift_on_aws/create-cluster-webui.png" width="300px" /></p>

<p>Your AWS account will be listed by its ID on the first page of the wizard.
Follow the prompts to configure and install a cluster.</p>

<p><a id="ipi"></a></p>

<h2 id="installer-provisioned-infrastructure-ipi">Installer-provisioned infrastructure (IPI)</h2>

<blockquote>
  <p><a href="https://github.com/joshgav/openshift-on-aws/tree/main/ipi">Follow along with scripts</a>.</p>
</blockquote>

<p>Even if your cluster won’t be managed by Red Hat it can still be deployed and
configured in AWS automatically with a short list of commands. This method will
set up EC2 machines and volumes and VPC networks as well as the cluster itself.
It’s known as “Installer-provisioned infrastructure” (IPI); here’s how
to do it.</p>

<h3 id="setup-1">Setup</h3>

<p>At the heart of IPI and other OpenShift deployment methods is the
<strong>openshift-install</strong> CLI tool. Download it from the <a href="https://console.redhat.com/openshift/downloads#tool-x86_64-openshift-install">downloads
section</a>
of the OpenShift console, or directly from
<a href="https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/stable/openshift-install-linux.tar.gz">https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/stable/openshift-install-linux.tar.gz</a>.
Its source is at <a href="https://github.com/openshift/installer">https://github.com/openshift/installer</a>. Run
<code class="language-plaintext highlighter-rouge">openshift-install create --help</code> for a list of installation steps, which by
default proceed as follows:</p>

<ol>
  <li><strong>install-config</strong>: Generate configuration manifest for infrastructure and cluster</li>
  <li><strong>manifests</strong>: Generate required Kubernetes resource manifests</li>
  <li><strong>ignition-configs</strong>: Embed Kubernetes resource manifests in OS configuration files</li>
  <li><strong>cluster</strong>: Provision EC2 machines and bootstrap them with the configuration files created previously</li>
</ol>

<p>You’ll also need a <strong>pull secret</strong> with credentials for Red Hat’s
container registries. Copy this from
<a href="https://console.redhat.com/openshift/downloads#tool-pull-secret">https://console.redhat.com/openshift/downloads#tool-pull-secret</a>.</p>

<p>A <strong>SSH key pair</strong> is required for access to provisioned machines; you’ll need
to provide its public key to the installer and save the private
key for access. You can copy an existing key from (for example)
<code class="language-plaintext highlighter-rouge">~/.ssh/id_rsa.pub</code> or create a new one in a secure place using (for example)
<code class="language-plaintext highlighter-rouge">ssh-keygen -t rsa -b 4096 -C "user@openshift" -f "./id_rsa" -N ''</code>.
Copy the contents of the <code class="language-plaintext highlighter-rouge">*.pub</code> file as the value of <code class="language-plaintext highlighter-rouge">SSH_PUBLIC_KEY</code> below and
save the private key for later.</p>

<p>Finally, to be able to access your cluster’s API server and web console by name
you’ll need an AWS <strong>Route53 public hosted zone</strong> for your cluster’s base domain
name. For example, I delegate a domain named <code class="language-plaintext highlighter-rouge">aws.joshgav.com</code> from my registrar GoDaddy
to a new AWS Route53 zone, see following screenshots. Specifically, after
creating the Route53 zone I create NS records for <code class="language-plaintext highlighter-rouge">aws</code> in the parent
<code class="language-plaintext highlighter-rouge">joshgav.com</code> zone at GoDaddy pointing to the name servers selected by Route53. More
details <a href="https://docs.openshift.com/container-platform/4.10/installing/installing_aws/installing-aws-account.html#installation-aws-route53_installing-aws-account">from RedHat
here</a>
and <a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/CreatingNewSubdomain.html">from AWS
here</a>.</p>

<p><img src="../assets/openshift_on_aws/aws-route53.png" width="300px" />
<img src="../assets/openshift_on_aws/godaddy-dns.png" width="300px" /></p>

<h3 id="create-cluster-1">Create cluster</h3>

<p>With these prerequisites in place, you’ll use <code class="language-plaintext highlighter-rouge">openshift-install create ...</code> to
manage phases of the installation process. An automatable approach is to define
the desired config of your cluster in a file named <code class="language-plaintext highlighter-rouge">install-config.yaml</code>, put it
in a directory <code class="language-plaintext highlighter-rouge">${WORKDIR}</code> and run the installer in the context of that
directory like so: <code class="language-plaintext highlighter-rouge">openshift-install create cluster --dir ${WORKDIR}</code>.
For example, following is a template <code class="language-plaintext highlighter-rouge">install-config.yaml</code> file; use your own values
for <code class="language-plaintext highlighter-rouge">OPENSHIFT_PULL_SECRET</code>, <code class="language-plaintext highlighter-rouge">YOUR_DOMAIN_NAME</code> and <code class="language-plaintext highlighter-rouge">SSH_PUBLIC_KEY</code> established
above.</p>

<blockquote>
  <p>The schema for <code class="language-plaintext highlighter-rouge">install-config.yaml</code> is in</p>
</blockquote>
<p><a href="https://github.com/openshift/installer/blob/master/pkg/types/installconfig.go">https://github.com/openshift/installer/blob/master/pkg/types/installconfig.go</a>.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">ipi</span>
<span class="na">baseDomain</span><span class="pi">:</span> <span class="s">${YOUR_DOMAIN_NAME}</span>
<span class="na">controlPlane</span><span class="pi">:</span>
  <span class="na">architecture</span><span class="pi">:</span> <span class="s">amd64</span>
  <span class="na">hyperthreading</span><span class="pi">:</span> <span class="s">Enabled</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">master</span>
  <span class="na">platform</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
<span class="na">compute</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">architecture</span><span class="pi">:</span> <span class="s">amd64</span>
  <span class="na">hyperthreading</span><span class="pi">:</span> <span class="s">Enabled</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">worker</span>
  <span class="na">platform</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
<span class="na">networking</span><span class="pi">:</span>
  <span class="na">networkType</span><span class="pi">:</span> <span class="s">OVNKubernetes</span>
  <span class="na">clusterNetwork</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">cidr</span><span class="pi">:</span> <span class="s">10.128.0.0/14</span>
    <span class="na">hostPrefix</span><span class="pi">:</span> <span class="m">23</span>
  <span class="na">machineNetwork</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">cidr</span><span class="pi">:</span> <span class="s">10.0.0.0/16</span>
  <span class="na">serviceNetwork</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">172.30.0.0/16</span>
<span class="na">platform</span><span class="pi">:</span>
  <span class="na">aws</span><span class="pi">:</span>
    <span class="na">region</span><span class="pi">:</span> <span class="s">us-east-1</span>
<span class="na">publish</span><span class="pi">:</span> <span class="s">External</span>
<span class="na">pullSecret</span><span class="pi">:</span> <span class="s1">'</span><span class="s">${OPENSHIFT_PULL_SECRET}'</span>
<span class="na">sshKey</span><span class="pi">:</span> <span class="s1">'</span><span class="s">${SSH_PUBLIC_KEY}'</span>
</code></pre></div></div>

<h3 id="monitor-installation-1">Monitor installation</h3>

<p>Cluster installation will take 30 minutes or more. Watch logs stream to stdout
following the <code class="language-plaintext highlighter-rouge">openshift-install create cluster</code> command, run <code class="language-plaintext highlighter-rouge">openshift-install wait-for install-complete</code>,
or tail the <code class="language-plaintext highlighter-rouge">.openshift_install.log</code> file in the installation working directory
to track progress of infrastructure and cluster installation.</p>

<p>Internally, <code class="language-plaintext highlighter-rouge">openshift-install</code> deploys infrastructure as described in <a href="https://github.com/openshift/installer/tree/master/data/data">these
Terraform configs</a>,
and many log entries come from Terraform.</p>

<h3 id="use-cluster-1">Use cluster</h3>

<p>A username and password for your cluster will be in the final lines of the
install log, either on stdout or in the <code class="language-plaintext highlighter-rouge">.openshift_install.log</code> file in the
working directory. In addition, a “kubeconfig” file and the kubeadmin user’s
password are saved in the <code class="language-plaintext highlighter-rouge">auth</code> directory of the installation dir. Login to
your cluster using one of the following mechanisms:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">## using kubeconfig with embedded certificate</span>
<span class="nb">export </span><span class="nv">KUBECONFIG</span><span class="o">=</span>temp/_workdir/auth/kubeconfig

<span class="c">## using username and password</span>
oc login <span class="nt">--user</span> kubeadmin <span class="nt">--password</span> <span class="s2">"</span><span class="si">$(</span><span class="nb">cat </span>temp/_workdir/auth/kubeadmin-password<span class="si">)</span><span class="s2">"</span>

<span class="c">## verify authorization</span>
oc get pods <span class="nt">-A</span>
</code></pre></div></div>

<p><a id="upi"></a></p>

<h2 id="user-provisioned-infrastructure-upi">User-provisioned infrastructure (UPI)</h2>

<blockquote>
  <p><a href="https://github.com/joshgav/openshift-on-aws/tree/main/upi">Follow along with scripts</a>.</p>
</blockquote>

<p>Though the easiest way to get started with OpenShift on AWS is with ROSA or
installer-provisioned infrastructure (IPI), Red Hat also enables you to deploy
and configure your own cloud infrastructure - machines, networks, and storage -
and provision a cluster on those via a method known as “user-provisioned
infrastructure” or UPI.</p>

<h3 id="setup-2">Setup</h3>

<p>Like IPI installations, UPI installations use <code class="language-plaintext highlighter-rouge">openshift-install</code> and the other
prerequisites to generate resource manifests and OS configuration files. Unlike
IPI though, it’s up to the user to configure machines and supply these files to
them when needed. For example, in AWS this is often accomplished by putting
configurations in a S3 bucket and asking machines to retrieve them from the
bucket’s URL on startup.</p>

<h3 id="create-cluster-2">Create cluster</h3>

<p>To guide users in provisioning their own infrastructure Red Hat provides a set
of CloudFormation templates reflecting good patterns for OpenShift clusters; these
templates are available
<a href="https://github.com/openshift/installer/tree/master/upi/aws/cloudformation">here</a>
and the example which accompanies this article uses them.</p>

<p>Creating a UPI cluster in AWS follows these high-level steps:</p>

<ol>
  <li>Initialize Kubernetes manifests and OS configurations with <code class="language-plaintext highlighter-rouge">openshift-install</code></li>
  <li>Deploy AWS networks and machines using recommended CloudFormation templates
or equivalent mechanisms, bootstrapping machines from generated configurations</li>
  <li>Await completed installation using <code class="language-plaintext highlighter-rouge">openshift-install</code></li>
</ol>

<p>The AWS resources recommended for OpenShift include a VPC and subnets, a DNS
zone and records, load balancers and target groups, IAM roles for EC2 instances,
security groups and even an S3 bucket. Several kinds of cluster nodes are also
included - bootstrap, control plane and worker. The bootstrap machine starts
first and installs the production cluster on the other machines.</p>

<p>Full instructions for AWS UPI are
<a href="https://docs.openshift.com/container-platform/4.10/installing/installing_aws/installing-aws-user-infra.html">here</a>.</p>

<h3 id="monitor-installation-2">Monitor installation</h3>

<p>Once EC2 instances have started and installation of bootstrap and production
clusters has begun, you can monitor progress using <code class="language-plaintext highlighter-rouge">openshift-install wait-for
[install-complete | bootstrap-complete]</code> in your working directory. As with
other methods installation will probably take more than 30 minutes.</p>

<p>One step in cluster provisioning is intentionally difficult to automate -
approving Certificate Signing Requests (CSRs) for nodes. Ideally an
administrator should verify the provenance of a CSR is the expected node prior
to approving the request. You can check if CSRs are awaiting approval with <code class="language-plaintext highlighter-rouge">oc
get csr</code>. Approve all pending requests with something like the following:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">csrs</span><span class="o">=(</span><span class="si">$(</span>oc get csr <span class="nt">-o</span> json | jq <span class="nt">-r</span> <span class="s1">'.items[] | select(.status == {}) | .metadata.name'</span><span class="si">)</span><span class="o">)</span>
<span class="k">for </span>csr <span class="k">in</span> <span class="s2">"</span><span class="k">${</span><span class="nv">csrs</span><span class="p">[@]</span><span class="k">}</span><span class="s2">"</span><span class="p">;</span> <span class="k">do
    </span>oc adm certificate approve <span class="s2">"</span><span class="k">${</span><span class="nv">csr</span><span class="k">}</span><span class="s2">"</span>
<span class="k">done</span>
</code></pre></div></div>

<p>In the sample scripts which accompany this article this check runs in the
background while the cluster is being provisioned so that CSRs are immediately
approved.</p>

<h3 id="use-cluster-2">Use cluster</h3>

<p>With UPI you don’t use the <code class="language-plaintext highlighter-rouge">openshift-install create cluster</code> command; instead
run <code class="language-plaintext highlighter-rouge">openshift-install wait-for install-complete</code> after the installation process
has started. Like with IPI you can monitor stdout of this command and/or the
<code class="language-plaintext highlighter-rouge">.openshift_install.log</code> file for info on progress of cluster installation. When
the cluster is ready, log in as above with <code class="language-plaintext highlighter-rouge">oc login</code> as the kubeadmin user with
the password in <code class="language-plaintext highlighter-rouge">${workdir}/auth/kubeadmin-password</code>, or set your KUBECONFIG env
var to the path <code class="language-plaintext highlighter-rouge">${workdir}/auth/kubeconfig</code>.</p>

<p>Once ready reach the console of your cluster at
<code class="language-plaintext highlighter-rouge">https://console-openshift-console.apps.${CLUSTER_NAME}.${BASE_DOMAIN}/</code>.</p>

<p><a id="kubespray"></a></p>

<h2 id="kubespray">Kubespray</h2>

<blockquote>
  <p><a href="https://github.com/joshgav/openshift-on-aws/tree/main/kubespray">Follow along with scripts</a>.</p>
</blockquote>

<p>The previous sections described how to deploy OpenShift, Red Hat’s Kubernetes
distribution, on Amazon Web Services with various levels of support and
automation for provisioning and operation. In this section we’ll deploy upstream
Kubernetes using <a href="https://kubespray.io">Kubespray</a> to compare, contrast and
gather new ideas. Notably, kubespray’s included configuration for AWS
infrastructure yields the following environment, nearly identical to that
produced by openshift-install and ROSA.</p>

<p><img src="../assets/openshift_on_aws/aws-kubespray.png" width="400px" alt="kubespray AWS infrastructure; credit: https://github.com/kubernetes-sigs/kubespray/blob/master/contrib/terraform/aws/docs/aws_kubespray.png" /></p>

<p>As with user-provisioned infrastructure (UPI) for OpenShift, with Kubespray the
user first installs infrastructure as they will (e.g. with Terraform or
CloudFormation) and then uses Kubespray to install a cluster on that
infrastructure. Kubespray offers Terraform configurations for deploying typical
environments in cloud providers; for this example I used the <a href="https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/aws">configurations for
AWS</a>.</p>

<blockquote>
  <p>Note: The most basic cluster installation tool is
<a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">kubeadm</a>,
but it leaves many critical aspects of the cluster incomplete, such as a network
overlay, container registry and load balancer controller.
<a href="https://kubernetes.io/docs/setup/production-environment/tools/kubespray/">Kubespray</a>
is also maintained by the Kubernetes project and provides a more complete
deployment.</p>
</blockquote>

<h2 id="create-cluster-3">Create cluster</h2>

<p>The last step of infrastructure provisioning creates an inventory file for
Ansible to consume to deploy cluster components. When infrastructure is deployed
and that file is ready run the main Kubespray process - an Ansible playbook -
using that inventory, e.g.: <code class="language-plaintext highlighter-rouge">ansible-playbook -i hosts.ini cluster.yaml</code>.
Customize the deployment by changing variables in inventory vars files or by
passing <code class="language-plaintext highlighter-rouge">-e key=value</code> pairs to the ansible-playbook invocation. Check out the
the <code class="language-plaintext highlighter-rouge">deploy-cluster.sh</code> script in the walkthrough for examples.</p>

<p>Rather than installing the Kubespray Ansible environment locally, you may prefer
to run commands in a containerized process bound to your inventory and SSH
files, for example as follows:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>podman run <span class="nt">--rm</span> <span class="nt">-it</span> <span class="se">\</span>
    <span class="nt">--mount</span> <span class="nb">type</span><span class="o">=</span><span class="nb">bind</span>,source<span class="o">=</span>kubespray/inventory/cluster,dst<span class="o">=</span>/inventory,relabel<span class="o">=</span>shared <span class="se">\</span>
    <span class="nt">--mount</span> <span class="nb">type</span><span class="o">=</span><span class="nb">bind</span>,source<span class="o">=</span>.ssh/id_rsa,dst<span class="o">=</span>/root/.ssh/id_rsa,relabel<span class="o">=</span>shared <span class="se">\</span>
        quay.io/kubespray/kubespray:v2.19.0 <span class="se">\</span>
            bash

<span class="c"># when prompted, enter (for example):</span>
ansible-playbook cluster.yml <span class="se">\</span>
    <span class="nt">-i</span> /inventory/hosts.ini <span class="se">\</span>
    <span class="nt">--private-key</span> /root/.ssh/id_rsa <span class="se">\</span>
    <span class="nt">--become</span> <span class="nt">--become-user</span><span class="o">=</span>root <span class="se">\</span>
    <span class="nt">-e</span> <span class="s2">"kube_version=v1.23.7"</span> <span class="se">\</span>
    <span class="nt">-e</span> <span class="s2">"ansible_user=ec2-user"</span> <span class="se">\</span>
    <span class="nt">-e</span> <span class="s2">"kubeconfig_localhost=true"</span>
</code></pre></div></div>

<h3 id="use-cluster-3">Use cluster</h3>

<p>The Ansible variable <code class="language-plaintext highlighter-rouge">kubeconfig_localhost=true</code> indicates that a kubeconfig
file with credentials for the provisioned cluster should be written to the
inventory directory once the cluster is ready. You can use this config to
authenticate to the cluster’s API server and invoke <code class="language-plaintext highlighter-rouge">kubectl</code> commands.</p>

<p>Initially the kubeconfig file will use an AWS-internal IP address for the
location of the API server. To manage your cluster from outside AWS you’ll need
to change the server’s URL to the address of your externally-accessible load
balancer (provisioned by Terraform previously). You can find the external load
balancer URL with the command <code class="language-plaintext highlighter-rouge">aws elbv2 describe-load-balancers --output json | jq -r '.LoadBalancers[0].DNSName'</code>.
Replace the server URL in the kubeconfig file with this hostname, being sure to
prepend <code class="language-plaintext highlighter-rouge">https://</code> and append <code class="language-plaintext highlighter-rouge">:6443/</code>.</p>

<p>Finally, set your KUBECONFIG env var to the file’s path, i.e.
<code class="language-plaintext highlighter-rouge">export KUBECONFIG=inventory/cluster/artifacts/admin.conf</code>
and then run <code class="language-plaintext highlighter-rouge">kubectl get pods -A</code>. If all is well you should get a list of all
pods in the cluster.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this article and <a href="https://github.com/joshgav/openshift-on-aws.git">accompanying
code</a> we’ve discussed and
demonstrated how to deploy an OpenShift or upstream Kubernetes cluster in AWS
using four different methods which progress from simplest with least control to
most complex yet customizable: ROSA -&gt; IPI -&gt; UPI -&gt; Kubespray.</p>

<p>To minimize the complexity and overhead of managing your own clouds and
clusters, start with the simplest method - ROSA - and progress to others as
greater control and customization is needed.</p>

<p>Please provide feedback in <a href="https://github.com/joshgav/openshift-on-aws/">the
repo</a> or on
<a href="https://twitter.com/joshugav">Twitter</a>. Thank you!</p>]]></content><author><name>Josh Gavant</name><email>joshgavant@gmail.com</email></author><category term="kubernetes" /><category term="openshift" /><category term="installation" /><summary type="html"><![CDATA[Summary: Descriptions and simple scripts for supported ways to deploy OpenShift on Amazon Web Services - ROSA, IPI and UPI.]]></summary></entry><entry><title type="html">Clusters for all!</title><link href="https://blog.joshgav.com//posts/cluster-level-multitenancy" rel="alternate" type="text/html" title="Clusters for all!" /><published>2022-05-16T16:00:00+00:00</published><updated>2022-05-16T16:00:00+00:00</updated><id>https://blog.joshgav.com//posts/cluster-level-multitenancy</id><content type="html" xml:base="https://blog.joshgav.com//posts/cluster-level-multitenancy"><![CDATA[<p><img src="/assets/virtual_cluster/pleiades.jpg" alt="Pleiades star clusters" /></p>

<p>A decision which faces many large organizations as they adopt cloud architecture is how to provide isolated spaces within the same environments and clusters for various teams and purposes. For example, marketing and sales applications may need to be isolated from an organization’s customer-facing applications; and development teams building any app usually require extra spaces for tests and verification.</p>

<h2 id="namespace-as-unit-of-tenancy">Namespace as unit of tenancy</h2>

<p>To address this need, many organizations have started to use namespaces as units of isolation and tenancy, a pattern previously described by <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/multitenancy-overview">Google</a> and <a href="https://kubernetes.io/blog/2021/04/15/three-tenancy-models-for-kubernetes/">Kubernetes contributors</a>. But namespace-scoped isolation is often insufficient because some concerns are managed at cluster scope. In particular, installing new resource types (CRDs) is a cluster-scoped activity; and today independent teams often want to install custom resource types and operators. Also, more developers are themselves writing software operators and custom resource types and find themselves requiring cluster-scoped access for research and tests.</p>

<h2 id="cluster-as-unit-of-tenancy">Cluster as unit of tenancy</h2>

<p>For these reasons and others, tenants often require their own isolated clusters with unconstrained access rights. In an isolated cluster, a tenant gets its own Kubernetes API server and persistence store and fully manages all namespaces and custom resource types in its cluster.</p>

<p>But deploying physical or even virtual machines for many clusters is inefficient and difficult to manage, so organizations have struggled to provide clusters to tenant teams. Happily :smile:, to meet these organizations’ and users’ needs, leading Kubernetes vendors have been researching and developing lighter weight mechanisms to provide isolated clusters for an organization’s tenants. In this post we’ll compare and contrast several of these emergent efforts.</p>

<p>Do you have other projects and ideas to enhance multitenancy for cloud architecture? Then please join CNCF’s App Delivery advisory group in discussing these <a href="https://github.com/cncf/tag-app-delivery/issues/193">here</a>; thank you!</p>

<h3 id="vcluster">vcluster</h3>

<p><a href="https://www.vcluster.com/">vcluster</a> is <a href="https://www.google.com/search?q=vcluster&amp;tbm=nws">a prominent project</a> and CLI tool maintained by <a href="https://loft.sh/">loft.sh</a> that provisions a virtual cluster as a StatefulSet within a tenant namespace. Access rights from the hosting namespace are propogated to the hosted virtual cluster such that the namespace tenant becomes the cluster’s only tenant. As cluster admins, tenant members can create cluster-scoped resources like CRDs and ClusterRoles.</p>

<p>The virtual cluster runs its own Kubernetes API service and persistence store independent of those of the hosting cluster. It can be published by the hosting cluster as a LoadBalancer-type service and accessed directly with kubectl and other Kubernetes API-compliant tools. This enables users of the tenant cluster to work with it directly with little or no knowledge of its host.</p>

<p><img src="/assets/virtual_cluster/vcluster.png" alt="vcluster architecture" /></p>

<p>A high-level perspective of vcluster’s architecture.</p>

<p>In vcluster and the following solutions, the virtual cluster is a “metadata-only” cluster, in that resources in it are persisted to a backing store like etcd, but no schedulers act to reify the persisted resources - ultimately as pods. Instead, a “syncer” synchronization service copies and transforms reifiable resources - podspecs - from the virtual cluster to the hosting namespace of the hosting cluster. Schedulers in the hosting cluster then detect and reify these resources in the same underlying tenant namespace where the virtual cluster’s control plane runs.</p>

<p>An advantage of vcluster’s approach of scheduling pods in the hosting namespace is that the hosting cluster ultimately handles all workloads and applies namespace quotas - all work happens within the namespace allocated to the tenant by the hosting cluster administrator. A disadvantage is that schedulers cannot be configured in the virtual cluster since pods aren’t actually run there. (Update: vcluster now supports a virtual scheduler, see <a href="https://www.vcluster.com/docs/architecture/scheduling#separate-vcluster-scheduler">https://www.vcluster.com/docs/architecture/scheduling#separate-vcluster-scheduler</a>.)</p>

<ul>
  <li><a href="https://github.com/loft-sh/vcluster">vcluster on GitHub</a></li>
</ul>

<h3 id="cluster-api-provider-nested-capn">Cluster API Provider Nested (CAPN)</h3>

<p>In vcluster, bespoke support for control plane implementations is required; as of this writing, vcluster supports k3s, k0s and vanilla k8s distributions.</p>

<p>To support <em>any</em> control plane implementation, the <a href="https://github.com/kubernetes-sigs/cluster-api-provider-nested">Cluster API Provider Nested</a> project implements an architecture similar to that of vcluster, including a metadata-only cluster and a syncer, but provisions the control plane using a Cluster API provider rather than a bespoke distribution.</p>

<p><img src="/assets/virtual_cluster/capn.png" alt="capn architecture" /></p>

<p>A high-level perspective of CAPN’s architecture.</p>

<p>CAPN promises to enable control planes implementable via Cluster API to serve virtual clusters.</p>

<h3 id="hypershift">HyperShift</h3>

<p>Similar to the previous two, <a href="https://www.redhat.com/">Red Hat</a>’s <a href="https://github.com/openshift/hypershift">HyperShift</a> project provisions an OpenShift (Red Hat’s Kubernetes distro) control plane as a collection of pods in a host namespace. But rather than running workloads within the hosting cluster and namespace like vcluster, HyperShift control planes are connected to a pool of dedicated worker nodes where pods are synced and scheduled.</p>

<p><img src="/assets/virtual_cluster/hypershift.png" alt="HyperShift architecture" /></p>

<p>A high-level perspective of HyperShift’s architecture.</p>

<p>HyperShift’s model may be most appropriate for a hosting provider like Red Hat which desires to abstract control plane management from their customers and allow them to just manage worker nodes.</p>

<h3 id="kcp">kcp</h3>

<p>Finally, <a href="https://github.com/kcp-dev/kcp">kcp</a> is another proposal and project from <a href="https://www.redhat.com/">Red Hat</a> inspired by and reimagined from all of the previous ideas. Whereas the above virtual clusters run <em>within</em> a host cluster and turn to the host cluster to run pods, manage networks and provision volumes, kcp reverses this paradigm and makes the <em>hosting</em> cluster a metadata-only cluster. <em>Child</em> clusters - <em>workspaces</em> in the kcp project - are registered with the hub metadata-only cluster and work is delegated to these children based on labels on resources in the hub.</p>

<p><img src="/assets/virtual_cluster/kcp.png" alt="kcp architecture" /></p>

<p>A high-level perspective of kcp’s architecture.</p>

<p>As opposed to hosted virtual clusters, child clusters in kcp <em>could</em> manage their own schedulers. Another advantage of kcp’s paradigm inversion is centralized awareness and management of child clusters. In particular, this enables simpler centralized policies and standards for custom resource types to be propogated to all children.</p>

<h2 id="conclusion">Conclusion</h2>

<p>vcluster, CAPN, HyperShift, and kcp are emerging projects and ideas to meet cloud users’ needs for multitenancy with <em>clusters</em> as the unit of tenancy. Early adopters are already providing feedback on good and better parts of these approaches and new ideas emerge daily.</p>

<p>Want to help drive new ideas for cloud multitenancy? Want to help cloud users understand and give feedback on emerging paradigms in this domain? Then join <a href="https://github.com/cncf/tag-app-delivery/issues/193">the discussion</a> in CNCF’s TAG App Delivery. Thank you!</p>

<h4 id="colophon">Colophon</h4>

<p>The image at the top is of star clusters in <a href="https://en.wikipedia.org/wiki/Pleiades">Pleiades</a> and the picture was copied from that article.</p>]]></content><author><name>Josh Gavant</name><email>joshgavant@gmail.com</email></author><category term="multitenancy" /><category term="clusters" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Kubernetes isn’t about containers</title><link href="https://blog.joshgav.com//posts/kubernetes-isnt-about-containers" rel="alternate" type="text/html" title="Kubernetes isn’t about containers" /><published>2021-12-16T19:00:00+00:00</published><updated>2021-12-16T19:00:00+00:00</updated><id>https://blog.joshgav.com//posts/kubernetes-isnt-about-containers</id><content type="html" xml:base="https://blog.joshgav.com//posts/kubernetes-isnt-about-containers"><![CDATA[<p>It’s about APIs; we’ll get to that shortly.</p>

<h2 id="first-there-were-containers">First there were containers</h2>

<p>Now Docker <em>is</em> about containers: running complex software with a simple <code class="language-plaintext highlighter-rouge">docker run postgres</code> command was a revelation to software developers in 2013, unlocking agile infrastructure that they’d never known. And happily, as developers adopted containers as a standard build and run target, the industry realized that the same encapsulation fits nicely for workloads to be scheduled in compute clusters by orchestrators like Kubernetes and Apache Mesos. Containers have become the most important workload type managed by these schedulers, but as the title says that’s not what’s most valuable about Kubernetes.</p>

<p>Kubernetes is not about more general workload <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/">scheduling</a> either (sorry <a href="https://krustlet.dev/">Krustlet</a> fans). While scheduling various workloads efficiently is an important value Kubernetes provides, it’s not the reason for its success.</p>

<h2 id="then-there-were-apis">Then there were APIs</h2>

<p><img width="400" alt="Always has been APIs" src="/assets/always_has_been_apis.jpeg" /></p>

<p>Rather, the attribute of Kubernetes that’s made it so successful and valuable is that <strong>it provides a set of standard programming interfaces for writing and using software-defined infrastructure services</strong>. Kubernetes provides specifications and implementations - a complete framework - for designing, implementing, operating and using infrastructure services of all shapes and sizes based on the same core structures and semantics: typed resources watched and reconciled by controllers.</p>

<p>To elaborate, consider what preceded Kubernetes: a hodge-podge of hosted “cloud” services with different APIs, descriptor formats, and semantic patterns. We’d piece together compute instances, block storage, virtual networks and object stores in one cloud; and in another we’d create the same using entirely different structures and APIs. Tools like Terraform came along and offered a common format across providers, but the original structures and semantics remained as variegated as ever - a Terraform descriptor targeting AWS stands no chance in Azure!</p>

<p>Now consider what Kubernetes provided from its earliest releases: standard APIs for describing compute requirements as pods and containers; virtual networking as services and eventually ingresses; persistent storage as volumes; and even workload identities as attestable service accounts. These formats and APIs work smoothly within Kubernetes distributions running everywhere, from public clouds to private datacenters. Internally, each provider maps the Kubernetes structures and semantics to that hodge-podge of native APIs mentioned in the previous paragraph.</p>

<p>Kubernetes offers a standard interface for managing software-defined infrastructure - <a href="https://joshgav.github.io/2021/09/30/cloud-redefined-infrastructure.html">cloud</a>, in other words. <strong>Kubernetes is a standard API framework for cloud services.</strong></p>

<h2 id="and-then-there-were-more-apis">And then there were more APIs</h2>

<p>Providing a fixed set of standard structures and semantics is the foundation of Kubernetes’ success. Following on this, its next act is to extend that structure to <em>any and all</em> infrastructure resources. <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resource Definitions</a> (CRDs) were introduced in version 1.7 to allow other types of services to reuse Kubernetes’ programming framework. CRDs make it possible to request not only predefined compute, storage and network services from the Kubernetes API, but also databases, task runners, message buses, digital certificates, and whatever else a provider can imagine!</p>

<p>As providers have sought to offer their services via the Kubernetes API as custom resources, the <a href="https://operatorframework.io/">Operator Framework</a> and related projects from <a href="https://github.com/kubernetes/community/tree/master/sig-api-machinery">SIG API Machinery</a> have emerged to provide tools and guidance that minimize work required and maximize standardization across all these shiny new resource types. Projects like <a href="https://crossplane.io">Crossplane</a> have formed to map other provider resources like RDS databases and SQS queues into the Kubernetes API just like network interfaces and disks are handled by core Kubernetes controllers today. And Kubernetes distributors like <a href="https://cloud.google.com/blog/topics/developers-practitioners/build-platform-krm-part-2-how-kubernetes-resource-model-works">Google</a> and <a href="https://docs.openshift.com/container-platform/4.9/operators/understanding/crds/crd-managing-resources-from-crds.html">Red Hat</a> are providing more and more custom resource types in their base Kubernetes distributions.</p>

<p>All of this isn’t to say that the Kubernetes API framework is perfect. Rather it’s to say that <em>it doesn’t matter</em> (much) because the Kubernetes model has become a de facto standard. Many developers understand it, many tools speak it, and many providers use it. Even with warts, Kubernetes’ broad adoption, user awareness and interoperability mostly outweigh other considerations.</p>

<p>With the spread of the Kubernetes resource model it’s already possible to describe an entire software-defined computing environment as a collection of Kubernetes resources. Like running a single artifact with <code class="language-plaintext highlighter-rouge">docker run ...</code>, distributed applications can be deployed and run with a simple <code class="language-plaintext highlighter-rouge">kubectl apply -f ...</code>. And unlike the custom formats and tools offered by individual cloud service providers, the Kubernetes’ descriptors are much more likely to run in many different provider and datacenter environments, because <strong>they all implement the same APIs</strong>.</p>

<p>Kubernetes isn’t about containers after all. It’s about APIs.</p>]]></content><author><name>Josh Gavant</name><email>joshgavant@gmail.com</email></author><category term="containers" /><category term="kubernetes" /><category term="apis" /><category term="infrastructure" /><summary type="html"><![CDATA[It’s about APIs; we’ll get to that shortly.]]></summary></entry><entry><title type="html">Cloud redefines enterprise infrastructure</title><link href="https://blog.joshgav.com//posts/cloud-redefined-infrastructure" rel="alternate" type="text/html" title="Cloud redefines enterprise infrastructure" /><published>2021-09-30T13:00:00+00:00</published><updated>2021-09-30T13:00:00+00:00</updated><id>https://blog.joshgav.com//posts/cloud-redefined-infrastructure</id><content type="html" xml:base="https://blog.joshgav.com//posts/cloud-redefined-infrastructure"><![CDATA[<p><img src="/assets/infra_desired_state.png" alt="infrastructure platform" /></p>

<p>Cloud computing has been around for well over a decade, so we ought to know what “cloud” is by now. Indeed we understand its attributes well, such as flexibility, efficiency, connectivity, and scalability; in the <a href="https://github.com/cncf/toc/blob/main/DEFINITION.md">words of the CNCF</a> (emphasis added):</p>

<blockquote>
  <p>Cloud native technologies empower organizations to build and run <strong>scalable</strong> applications in modern, <strong>dynamic</strong> environments…  These techniques enable loosely coupled systems that are <strong>resilient</strong>, <strong>manageable</strong>, and <strong>observable</strong>… They allow engineers to make high-impact changes <strong>frequently</strong> and predictably with minimal toil.</p>
</blockquote>

<p>But what is “cloud” itself, provider of all these desirable attributes? Particularly in our era of hybrid, multi, and private “clouds” we should clearly define the term. And how does “cloud” relate to the rest of our infrastructure?</p>

<h2 id="what-is-cloud">What is cloud?</h2>

<p>Let us describe “cloud” by induction from existing ones: a <strong>cloud</strong> is a collection of automatable <em>infrastructure services</em> managed via a consistent set of interfaces - APIs, Web UIs, and CLIs. An <strong>infrastructure service</strong> is a service which serves other services which in turn serve users - an infrastructure service serves end users only indirectly. Stated a bit differently, <strong>an infrastructure service serves applications</strong> and their developers and <strong>a cloud is a collection of such services</strong>.</p>

<p>Per this definition, any provider of infrastructure services might be considered a cloud provider, though generally we reserve the title for providers that offer many diverse service types. “Multi-cloud” environments are those using infrastructure services from several providers. A “private cloud” is a collection of infrastructure services offered via an internal, perhaps custom interface.</p>

<p>With this in mind let’s now describe how cloud is changing enterprise infrastructure. The graphic above illustrates these.</p>

<h3 id="from-servers-to-serverless">From servers to serverless</h3>

<p>Before “cloud” many infrastructure specialists managed hardware servers, datacenters, network devices and operating system configurations. But “cloud” is replacing most such physical components with programmable, software-defined ones - virtual machines, virtual networks, dynamic datastores and queues, and much more. The job of infrastructure specialists is now to <strong>program virtual infrastructure</strong>.</p>

<p>A consistent, thorough, app-centric interface is one reasonable ultimate realization of cloud as defined, so “serverless” is a good example of a cloud interface for infrastructure driven strictly by app requirements. The most common paradigm though for programming virtual infrastructure today is to virtualize and automate existing architectural patterns - such as describing a router, a pod, and a datastore as software-defined Kubernetes resources and having a controller reify them.</p>

<p>Whatever the interface though, <strong>infrastructure is not about hardware anymore, it’s about software</strong>.</p>

<h3 id="from-services-to-platform">From services to platform</h3>

<p>Another change is that before “cloud” infrastructure services were often offered by different teams via different interfaces. An identity or TLS certificate was issued by one team; another would provision a collector for logs and metrics; and another would deploy servers, networks and operating systems. Each would collect metadata and implement requirements from app teams in their own ways.</p>

<p>But as infrastructure services become software-defined, interfaces and processes for acquiring those services can become more consistent, easier and faster for apps and developers to use and manage. For example, a set of Kubernetes resources or Terraform manifests could describe every infrastructure service required by an application.</p>

<p>In other words, cloud is an opportunity to bring together a bunch of disparate services and interfaces into a consistent <strong>platform</strong>.</p>

<h3 id="from-dependency-to-partner">From dependency to partner</h3>

<p>As infrastructure becomes more flexible and more capable of quickly fulfilling app developers’ needs, infrastructure teams are enabled to partner and agilely develop services together with application developers. The agility enabled by software allows infrastructure teams to deliver their set of services as a cohesive product to app developers, gathering and iterating quickly on feedback and new requirements.</p>

<p>With cloud, <strong>infrastructure becomes an active partner to product teams in delivering business value</strong> to customers and reacting to new circumstances.</p>

<h2 id="conclusion">Conclusion</h2>

<p>A “cloud” is <em>not</em> just something run by big tech companies like Microsoft and Amazon. Rather, “cloud” is <em>the</em> new paradigm of enterprise infrastructure itself: providing a consistent collection of automatable infrastructure services to apps and developers.</p>

<p>How are you providing “cloud” at your organization?</p>]]></content><author><name>Josh Gavant</name><email>joshgavant@gmail.com</email></author><category term="platform" /><category term="infrastructure" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Deliver platform capabilities</title><link href="https://blog.joshgav.com//posts/deliver-platform-capabilities" rel="alternate" type="text/html" title="Deliver platform capabilities" /><published>2021-08-30T13:00:00+00:00</published><updated>2021-08-30T13:00:00+00:00</updated><id>https://blog.joshgav.com//posts/deliver-platform-capabilities</id><content type="html" xml:base="https://blog.joshgav.com//posts/deliver-platform-capabilities"><![CDATA[<p><img src="/assets/city-platform.jpg" alt="Platform and capabilities" /></p>

<p>Platform teams exist to develop and manage capabilities required across many application service teams. When functioning well, platform teams provide expertise and support for complex capabilities which would otherwise have required such support in each application service team.</p>

<p>This post describes how a platform team may develop and manage these <strong>platform capabilities</strong> and gradually evolve a capabilities development and delivery <em>framework</em>. The framework should not be designed up front, but platform architects should help it emerge by building prototypical application services and developing initial capabilities for them, as described herein.</p>

<h2 id="develop-prototype-application-services">Develop prototype application services</h2>

<p>A prerequisite for developing a platform-level shared capability is a representative prototype of the application services wherein the capability will be used. For example, to research and develop service-to-service authentication and authorization mechanisms, two communicating application-level services must exist first; to develop a traffic management capability, one must have application services to route traffic to.</p>

<p>And so we come to the first step in developing platform capabilities: <strong>platform developers and application developers must cooperate to develop prototype application services that are truly representative of application-level scenarios</strong>. Not only must the two groups develop <em>initial</em> prototypes, they must also continuously improve and expand such prototypes to cover more scenarios and adapt to inevitable changes in the organization’s environment.</p>

<p>Architects and product managers should do the following to develop prototype application services:</p>

<ul>
  <li>Interview leads, reverse-engineer codebases and conduct experiments to identify and prioritize the most important and most typical scenarios and required capabilities for the organization’s application services</li>
  <li>Ensure all prototype application service code is parameterized and names are extracted</li>
  <li>Ensure that any senior developer can automatically reify a development environment with a single command; and conduct research and development using the prototypes</li>
</ul>

<p>Several iterations with several application service teams and platform capability teams will probably be required to refine prototype code and ensure coverage of essential scenarios.</p>

<h2 id="develop-and-deliver-capabilities">Develop and deliver capabilities</h2>

<p>As development of prototype application services begins, development of initial individual capabilities may also begin, as well as early planning for a standard capability development and delivery framework. Early capability development should focus on individual capabilities rather than a general framework; experiments using these initial individual capabilities will guide and inform planning and design of the greater framework. Just as prototype application services guide development of individual capabilities, so too individual capabilities guide development of a capability framework.</p>

<h3 id="develop-capabilities">Develop capabilities</h3>

<p>Capabilities should be developed and released as follows:</p>

<ol>
  <li>Define or refine definition of desired capability</li>
  <li>Research and develop implementations for capability</li>
  <li>Deliver capability</li>
  <li>Gather feedback and learnings and go to 1</li>
</ol>

<p>For example, a capability for managing secret configuration might follow this storyline:</p>

<ol>
  <li>Define desired capability:
    <ul>
      <li>inject secret configuration as key-value pairs at service start time</li>
    </ul>
  </li>
  <li>Research and develop implementations for capability
    <ul>
      <li>inject Vault agent configuration as pod sidecar using Helm charts</li>
      <li>deploy Vault admission controller system</li>
      <li>integrate Vault secrets with Kubernetes Secrets using <a href="https://secrets-store-csi-driver.sigs.k8s.io/">Secrets Store CSI Driver</a></li>
    </ul>
  </li>
  <li>Deliver capability
    <ul>
      <li>document how to configure Vault agent in a pod sidecar</li>
      <li>automatically configure Vault agent with default configuration via a Helm chart</li>
      <li>automatically configure Vault agent via an admission controller</li>
    </ul>
  </li>
  <li>Gather feedback and learnings and iterate
    <ul>
      <li>adjust exposed configuration options and APIs</li>
      <li>change implementation</li>
      <li>support TLS secrets specially</li>
    </ul>
  </li>
</ol>

<p>Every capability team will require its members to have expert-level domain knowledge in that capability, alleviating the need for developers in application teams to be expert in the domain. Developers on the team will develop the internal implementation of the capability based on their expertise in it, application service requirements, and the constraints imposed by the organization’s environment. PMs will share that knowledge with other partners and users. Team members will participate in external industry communities related to their domain as well to keep abreast of new opportunities and developments.</p>

<p>Capabilities themselves may be implemented in many ways. They have often been built as programming language libraries to be imported into applications in source code, at build time and/or at run time. In cloud-native applications, capabilities are often built as independent processes which communicate with a service’s main process via transports like HTTP, GRPC or TCP. In Kubernetes, supporting processes are typically deployed as sidecars in a pod or daemonsets on every node. Mutating admission controllers can also modify capabilities of a pod by modifying their configuration.</p>

<p>As capabilities are developed, a development framework should also emerge to assist with and coordinate fruitful patterns and should include the following:</p>

<ul>
  <li>Several viable options and examples for developing platform capabilities within the organization, such as operators, templating tools, daemonsets or pod sidecars</li>
  <li>Ability to provision a productive development environment for app and platform research and development</li>
</ul>

<h3 id="deliver-capabilities">Deliver capabilities</h3>

<p>When ready, platform teams must package and deliver their capability to be used by application teams. To this end, platform architects must gradually develop and manage a standard framework to guide capability developers in delivering and supporting their capabilities, and to provide a rational, consistent experience for application developers using them.</p>

<p>Once a given capability has been initially developed, it should be integrated and tested in application services in the following progression, which a framework should emerge to guide and facilitate:</p>

<ol>
  <li>Document required capability configuration and inform users how to set up the capability manually
    <ol>
      <li>Verify that application developers are satisfied with the <em>functionality</em> of the capability</li>
    </ol>
  </li>
  <li>Inject capability and default configuration via toggles and tags
    <ol>
      <li>Verify that application developers are satisfied with the exposed configuration options</li>
    </ol>
  </li>
  <li>Inject capability automatically and transparently
    <ol>
      <li>At build time with e.g., <code class="language-plaintext highlighter-rouge">helm</code>, <code class="language-plaintext highlighter-rouge">kustomize</code></li>
      <li>At deploy time with e.g., mutating admission controllers</li>
      <li>At run time with e.g., daemonsets and network proxies</li>
    </ol>
  </li>
</ol>

<p>Just as <em>application</em> delivery is only complete when customers begin using the application, platform <em>capability</em> delivery is only complete when application teams begin using the capability in production. The top goal of a platform capability framework should be to provide guided, standard ways to deliver platform capabilities to application developers. As individual capabilities are developed and tested, good general designs and strategies for capability delivery and integration will emerge and should influence development and evolution of the general framework.</p>

<p>Dimensions a platform capability delivery framework should consider making possible include the following:</p>

<ul>
  <li>To enable the capability, must the capability team team a) deploy and manage a service such as an operator; or b) package and publish a library; or c) something else?</li>
  <li>To enable a capability, must application developers a) explicitly integrate the capability in source code or infrastructure configuration or b) is it transparently injected?</li>
  <li>May application developers a) toggle and configure a capability or b) not?</li>
</ul>

<h2 id="summary">Summary</h2>

<p>Platform teams promise to multiply the efficiency of application service teams by centralizing knowledge about and management of shared application capabilities like identity and observability. Help both platform and application teams succeed by providing platform capability development and delivery frameworks to guide them.</p>]]></content><author><name>Josh Gavant</name><email>joshgavant@gmail.com</email></author><category term="platform" /><category term="infrastructure" /><category term="architecture" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">How container streaming (exec, port-forward) works in Kubernetes</title><link href="https://blog.joshgav.com//posts/container-streaming" rel="alternate" type="text/html" title="How container streaming (exec, port-forward) works in Kubernetes" /><published>2020-09-16T16:30:00+00:00</published><updated>2020-09-16T16:30:00+00:00</updated><id>https://blog.joshgav.com//posts/container-streaming</id><content type="html" xml:base="https://blog.joshgav.com//posts/container-streaming"><![CDATA[<h1 id="overview-of-kubelet">Overview of Kubelet</h1>

<p>Kubernetes’ <strong>kubelet</strong> is a server and controller which runs on every cluster node as an agent to allocate compute, storage and network resources for workloads described by <strong>PodSpec</strong>s retrieved from the API server. Not only does the kubelet manage pods for Kubernetes in “connected” mode, but it can also (or alternatively) read PodSpecs from the local filesystem or an HTTP endpoint in “standalone” mode. In short, the kubelet is an independent implementation of PodSpec.</p>

<blockquote>
  <p>For more on kubelet’s standalone mode check out <a href="https://coreos.com/blog/introducing-the-kubelet-in-coreos.html">this article</a> and <a href="https://github.com/kelseyhightower/standalone-kubelet-tutorial">this tutorial</a> from Kelsey Hightower.</p>
</blockquote>

<p>Kubelet handles the heavy lifting of provisioning virtual networks, allocating and attaching block storage and running container images by calling a <strong>container runtime</strong> like Docker via Kubelet’s Container Runtime Interface (CRI), as defined in <a href="https://github.com/kubernetes/cri-api/blob/master/pkg/apis/runtime/v1alpha2/api.proto">this protobuf spec</a>. A shim for CRI from Docker’s native API is <a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/kubelet/dockershim">included</a> in the kubelet, or you can follow <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">these instructions</a> to install and use another runtime like CRI-O.</p>

<p>Amongst the procedures offered by CRI’s <a href="https://github.com/kubernetes/cri-api/blob/205a053b09eb766d86191392b3e6bd94df6ceb0c/pkg/apis/runtime/v1alpha2/api.proto#L33-L110">RuntimeService</a> one finds <strong>Exec</strong>, <strong>Attach</strong> and <strong>PortForward</strong>, likely familiar to anyone who works with containers. These ultimately are core to the <code class="language-plaintext highlighter-rouge">kubectl exec ...</code>, <code class="language-plaintext highlighter-rouge">kubectl run -it ...</code>, <code class="language-plaintext highlighter-rouge">kubectl port-forward</code> and even the new <code class="language-plaintext highlighter-rouge">kubectl [alpha] debug ...</code> commands that container developers know and love. Following is how these commands and procedures work together to connect your terminal to a process in a worker node.</p>

<h1 id="exec">exec</h1>

<p>First let’s walk through what happens when you run <code class="language-plaintext highlighter-rouge">kubectl exec -it ${pod_name} sh --container ${container_name}</code> to run a shell in the context of an existing container. We’ll borrow and refer to the following diagram from <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/20191205-container-streaming-requests.md">this k8s enhancement proposal</a>.</p>

<p><img src="https://raw.githubusercontent.com/kubernetes/enhancements/master/keps/sig-node/1558-streaming-proxy-redirects/kubelet-proxied-streaming-request-sequence.png" style="margin-left: 40px;" /></p>

<h2 id="1-client">1. client</h2>

<p>Based on its arguments, <code class="language-plaintext highlighter-rouge">kubectl exec</code> builds a URL for and opens an HTTP/2 connection with the API server. The local terminal’s standard I/O streams (stdin, stdout, stderr) are connected to this transport. The URL formed for the API server is <code class="language-plaintext highlighter-rouge">http[s]://${api_server}/ns/${pod_namespace}/pods/${pod_name}/exec?stdin=true&amp;stdout=true&amp;stderr=true&amp;tty=true&amp;container=${container_name}&amp;command=sh</code>.</p>

<h3 id="source-refs">Source refs:</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">kubectl exec</code> [<a href="https://github.com/kubernetes/kubectl/blob/d70ead5fcaa0e8f8246715584147ba3bfd081411/pkg/cmd/exec/exec.go">1</a>]</li>
</ul>

<h2 id="2-apiserver">2. apiserver</h2>

<p>The corev1/pods APIService accepts the incoming request and handles it per <a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/registry/core/pod">its registration</a>. Specifically, it discovers the address and port of the Node/Kubelet running the indicated container and opens a streaming proxy connection to it. This stream is bound to the streams from the incoming request.</p>

<p>The URL for the kubelet server is of form <code class="language-plaintext highlighter-rouge">http[s]://${node_ip}:${kubelet_port}/${subresource}/${pod_namespace}/${pod_name}/${container_name}</code> where <code class="language-plaintext highlighter-rouge">${subresource}</code> can be <code class="language-plaintext highlighter-rouge">exec</code>, <code class="language-plaintext highlighter-rouge">attach</code>, <code class="language-plaintext highlighter-rouge">portforward</code> or a few others.</p>

<h3 id="source-refs-1">Source refs:</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">registry/core/pod.streamLocation</code> [<a href="https://github.com/kubernetes/kubernetes/blob/9621ac6ec7eddccdf007c043272c81b23408704b/pkg/registry/core/pod/strategy.go#L506-L511">1</a>]</li>
</ul>

<h2 id="3-kubelet">3. kubelet</h2>

<p>The kubelet provides its own API server which accepts the incoming request from the API server and forwards it to the container runtime. The kubelet then continues to proxy I/O streams between the API server and the container runtime. An option exists to hand off the stream with the container runtime directly to the API server (rather than continuing to proxy it through kubelet), but it has been deprecated.</p>

<p>The exec, attach, port-forward and logs actions are handled by Kubelet’s “debugging handlers.” They can be disabled by setting <a href="https://github.com/kubernetes/kubelet/blob/f87179761b5b3b817cf86fdf2e31801c61a8db7e/config/v1beta1/types.go#L255-L262">EnableDebuggingHandlers</a> to <code class="language-plaintext highlighter-rouge">false</code> in the global kubelet configuration, or by setting the flag <code class="language-plaintext highlighter-rouge">--enable-debugging-handlers=false</code> on an individual kubelet. <strong>Note</strong> that this will disable container logs via <code class="language-plaintext highlighter-rouge">kubectl logs</code> as well!</p>

<h3 id="source-refs-2">Source refs:</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">kubelet/server/NewServer(enableDebuggingHandlers)</code> [<a href="https://github.com/kubernetes/kubernetes/blob/3d52b8b5d60e1f74f4207f1d046734878297e354/pkg/kubelet/server/server.go#L243-L253">1</a>]</li>
  <li><code class="language-plaintext highlighter-rouge">kubelet/server/server.InstallDebuggingHandlers</code> [<a href="https://github.com/kubernetes/kubernetes/blob/3d52b8b5d60e1f74f4207f1d046734878297e354/pkg/kubelet/server/server.go#L411">2</a>]</li>
  <li><code class="language-plaintext highlighter-rouge">kubelet/server/server.getExec</code> [<a href="https://github.com/kubernetes/kubernetes/blob/3d52b8b5d60e1f74f4207f1d046734878297e354/pkg/kubelet/server/server.go#L795-L821">3</a>]</li>
</ul>

<h2 id="4-cri">4. CRI</h2>

<p>Finally the container runtime - or runtime shim in Docker’s case - receives the request from kubelet and takes the steps necessary to create and execute a process in the namespaces and cgroups of the target container. In Docker this is achieved by calling <a href="https://pkg.go.dev/github.com/moby/moby/client#Client.ContainerExecCreate">github.com/moby/moby/client#Client.ContainerExecCreate</a>.</p>

<p>In truth <code class="language-plaintext highlighter-rouge">exec</code> itself can be executed without a persistent connection, in which case you wouldn’t be able to send stdin or receive stdout from the executed command. When you specify <code class="language-plaintext highlighter-rouge">-i -t</code> with <code class="language-plaintext highlighter-rouge">exec</code> an attach action is executed immediately after exec to provide a persistent connection.</p>

<h3 id="source-refs-3">Source refs:</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">kubelet/cri/streaming.NewServer</code> [<a href="https://github.com/kubernetes/kubernetes/blob/e83412c331ae72718a84623870c420e6daf58a25/pkg/kubelet/cri/streaming/server.go#L125-L133">1</a>]</li>
  <li><code class="language-plaintext highlighter-rouge">kubelet/cri/streaming/server.serveExec</code> [<a href="https://github.com/kubernetes/kubernetes/blob/e83412c331ae72718a84623870c420e6daf58a25/pkg/kubelet/cri/streaming/server.go#L265-L297">2</a>]</li>
  <li><code class="language-plaintext highlighter-rouge">kubelet/cri/streaming/remotecommand/ServeExec</code> [<a href="https://github.com/kubernetes/kubernetes/blob/e83412c331ae72718a84623870c420e6daf58a25/pkg/kubelet/cri/streaming/remotecommand/exec.go#L44">3</a>]</li>
  <li><code class="language-plaintext highlighter-rouge">kubelet/dockershim/NativeExecHandler.ExecInContainer</code> [<a href="https://github.com/kubernetes/kubernetes/blob/fe1aeff2d2341e3d9a553534c814ad40f8219e35/pkg/kubelet/dockershim/exec.go#L64">4</a>]</li>
  <li><code class="language-plaintext highlighter-rouge">kubelet/dockershim/libdocker/kubeDockerClient.StartExec</code> [<a href="https://github.com/kubernetes/kubernetes/blob/e83412c331ae72718a84623870c420e6daf58a25/pkg/kubelet/dockershim/libdocker/kube_docker_client.go#L461">5</a>]</li>
  <li><code class="language-plaintext highlighter-rouge">moby/moby/client/Client.ContainerExecCreate</code> [<a href="https://pkg.go.dev/github.com/moby/moby/client#Client.ContainerExecCreate">6</a>]</li>
</ul>

<h1 id="port-forward">port-forward</h1>

<p>Whereas exec and attach work in the context of a container, port-forward communicates with the “pod”, or more specifically with the pod’s network namespace. In Kubelet’s built-in Docker CRI shim, port forwarding is accomplished with the following command. The “sandbox” in CRI represents the pod context.</p>

<p><code class="language-plaintext highlighter-rouge">nsenter -t ${sandbox_pid} -n socat - TCP4:localhost:${target_port}</code></p>

<h3 id="source-refs-4">Source Refs</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">kubelet/cri/streaming/portforward.ServePortForward</code> [<a href="https://github.com/kubernetes/kubernetes/blob/e83412c331ae72718a84623870c420e6daf58a25/pkg/kubelet/cri/streaming/portforward/portforward.go#L36-L53">1</a>]</li>
  <li><code class="language-plaintext highlighter-rouge">kubelete/dockershim/streamingRuntime.portForward</code> [<a href="https://github.com/kubernetes/kubernetes/blob/e83412c331ae72718a84623870c420e6daf58a25/pkg/kubelet/dockershim/docker_streaming_others.go">2</a>]</li>
</ul>

<h1 id="logs">logs</h1>

<p>Requests for container logs also pass through the kubelet to the CRI and are streamed back to the client.</p>

<h3 id="source-refs-5">Source Refs</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">kubelet/server/server.InstallDebuggingHandlers</code> [<a href="https://github.com/kubernetes/kubernetes/blob/fd9828b02a786d4fa8d2add04c37e33a616d0087/pkg/kubelet/server/server.go#L482-L488">1</a>]</li>
  <li><code class="language-plaintext highlighter-rouge">kubelet/server/server.getContainerLogs</code> [<a href="https://github.com/kubernetes/kubernetes/blob/fd9828b02a786d4fa8d2add04c37e33a616d0087/pkg/kubelet/server/server.go#L595-L661">2</a>]</li>
  <li><code class="language-plaintext highlighter-rouge">dockershim/dockerService.GetContainerLogs</code> [<a href="https://github.com/kubernetes/kubernetes/blob/fe1aeff2d2341e3d9a553534c814ad40f8219e35/pkg/kubelet/dockershim/docker_legacy_service.go#L49-L92">3</a>]</li>
</ul>]]></content><author><name>Josh Gavant</name><email>joshgavant@gmail.com</email></author><category term="kubernetes" /><category term="kubelet" /><category term="exec" /><summary type="html"><![CDATA[Overview of Kubelet]]></summary></entry><entry><title type="html">Introducing Partly Cloudy</title><link href="https://blog.joshgav.com//posts/welcome-to-partly-cloudy" rel="alternate" type="text/html" title="Introducing Partly Cloudy" /><published>2020-04-17T16:21:49+00:00</published><updated>2020-04-17T16:21:49+00:00</updated><id>https://blog.joshgav.com//posts/welcome-to-partly-cloudy</id><content type="html" xml:base="https://blog.joshgav.com//posts/welcome-to-partly-cloudy"><![CDATA[<p>So called because I’ll be writing posts about cloud-native patterns, practices and tools.</p>

<p>I chose to start by maintaining the site with <a href="https://jekyllrb.com/">Jekyll</a>, though I almost reconsidered upon discovering that the latest version of Jekyll <a href="https://github.com/github/pages-gem/issues/651">isn’t fully supported</a> for automatically-built GitHub Pages sites. That led me to write <a href="https://github.com/joshgav/joshgav.github.io/blob/master/.github/workflows/publish-site.yaml">a GitHub Action to build and publish the site</a>, inspired by <a href="https://sujaykundu.com/blog/post/deploy-jekyll-using-github-pages-and-github-actions">this post</a>. Feeling confident with that in hand, I pushed the first version and was pleasantly surprised to find the GitHub Pages <em>did</em> automatically build the site. Good thing too cause my custom Action originally pushed the built site to the <code class="language-plaintext highlighter-rouge">gh-pages</code> branch, which wouldn’t work for a user’s top-level user site - <code class="language-plaintext highlighter-rouge">joshgav.github.io</code> in my case.</p>

<p>I wasn’t happy with the unpolished feel of the default <code class="language-plaintext highlighter-rouge">minima</code> theme, so I reviewed some others at <a href="https://jekyllthemes.io/">jekyllthemes.io</a>. That site offers Jekyll themes for $$ too; I’m not sure of its business model. In any case it helped me find a simple theme named “<a href="https://github.com/huangyz0918/moving">moving</a>” which I liked. I installed it by editing my config files, testing locally, and pushing the changes to GitHub. But… my site didn’t render and I received an email notification that GitHub Pages only works with <a href="https://pages.github.com/themes/">this subset of Jekyll themes</a>. Great, an opportunity to tweak and use the Action action!</p>

<p>So to use my chosen theme I went back and tweaked my action to listen for pushes to the <code class="language-plaintext highlighter-rouge">source</code> branch and push changes to the <code class="language-plaintext highlighter-rouge">master</code> branch, as required for the “top-level” GitHub Page. It seems to be working now.</p>

<p>Oh, if you’re wondering I chose Jekyll cause it’s an “elder statesman” of static site generation by now and has a community of users and plugin developers. That it’s the official SSG for GitHub Pages also lent it favor.</p>]]></content><author><name>Josh Gavant</name><email>joshgavant@gmail.com</email></author><category term="info" /><category term="blog" /><summary type="html"><![CDATA[So called because I’ll be writing posts about cloud-native patterns, practices and tools.]]></summary></entry></feed>