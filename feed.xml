<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="https://blog.joshgav.com//feed.xml" rel="self" type="application/atom+xml" /><link href="https://blog.joshgav.com//" rel="alternate" type="text/html" /><updated>2022-08-09T15:35:33+00:00</updated><id>https://blog.joshgav.com//feed.xml</id><title type="html">Partly Cloudy</title><subtitle>A blog about cloud infrastructure and cloud-native application patterns and practices.</subtitle><author><name>Josh Gavant</name></author><entry><title type="html">Run OpenShift on AWS</title><link href="https://blog.joshgav.com//openshift-on-aws" rel="alternate" type="text/html" title="Run OpenShift on AWS" /><published>2022-08-09T07:00:00+00:00</published><updated>2022-08-09T07:00:00+00:00</updated><id>https://blog.joshgav.com//openshift-on-aws</id><content type="html" xml:base="https://blog.joshgav.com//openshift-on-aws"><![CDATA[<p>OpenShift is a Kubernetes distribution with batteries included - it’s ready for
many use cases out of the box. For example, a default installation includes a
network provider (CNI), container builder and registry, external ingress
provider and a cluster lifecycle management system. Contrast this with a cluster
provisioned by
<a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/">kubeadm</a>
or even the more featureful
<a href="https://kubernetes.io/docs/setup/production-environment/tools/kubespray/">kubespray</a>,
where adding these and other critical features requires thoughtful and often
fragile integration.</p>

<p>Not only does OpenShift offer a turnkey, ready-to-use Kubernetes environment, it
also has become easier and easier to deploy over time. Gone are the days when
operators and infrastructure teams would have to pre-provision Linux nodes and
carefully configure cloud networks. Though it’s still possible to customize to
your (or your organization’s!) heart’s content, many use cases can be met with
just a few commands to spin up an OpenShift Kubernetes cluster on a public cloud
provider like Amazon Web Services.</p>

<p>In this article we’ll discuss three ways to deploy and run OpenShift on AWS,
then we’ll contrast these with deploying upstream Kubernetes with kubespray.
Follow along with code at <a href="https://github.com/joshgav/openshift-on-aws.git">https://github.com/joshgav/openshift-on-aws.git</a>.</p>

<h2 id="rosa-red-hat-openshift-service-on-aws">ROSA: Red Hat OpenShift Service on AWS</h2>

<p>Follow along with <a href="https://github.com/joshgav/openshift-on-aws/tree/main/rosa">the scripts</a>.</p>

<p>Let’s start with the simplest option: Red Hat OpenShift Service on AWS, ROSA.
Deployment of a ROSA cluster includes deployment and correct configuration of
required compute, network and storage resources in AWS EC2 in addition to a
fully-featured OpenShift Kubernetes cluster. As opposed to other options to be
discussed, a ROSA environment is fully supported by Red Hat’s operations teams -
open a ticket and an expert Red Hat SRE will attend to it quickly.</p>

<h3 id="setup">Setup</h3>

<p>At the core of the ROSA lifecycle is the <a href="https://docs.openshift.com/rosa/rosa_cli/rosa-get-started-cli.html">rosa
CLI</a>. Get it
from the <a href="https://console.redhat.com/openshift/downloads">Downloads</a> section of
the Red Hat Console or directly from
<a href="https://mirror.openshift.com/pub/openshift-v4/clients/rosa/latest/rosa-linux.tar.gz">https://mirror.openshift.com/pub/openshift-v4/clients/rosa/latest/rosa-linux.tar.gz</a>.</p>

<p>You’ll need both Red Hat and AWS credentials to enable the <code class="highlighter-rouge">rosa</code> CLI to
provision and connect to resources. Your AWS credentials can be specified as
exported <code class="highlighter-rouge">AWS_ACCESS_KEY_ID</code>, <code class="highlighter-rouge">AWS_SECRET_ACCESS_KEY</code> and <code class="highlighter-rouge">AWS_REGION</code>
environment variables as <a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html">for the AWS
CLI</a>.</p>

<p>To get a token to login to your Red Hat account, click “View API token” at the
bottom of the Downloads page as shown in the following screenshot, or go
straight to <a href="https://console.redhat.com/openshift/token">the token page</a>. On
that page click “Load token”, then copy the raw token (not the <code class="highlighter-rouge">ocm</code> command
line) and run <code class="highlighter-rouge">rosa login --token="${your_token}"</code>. If successful you will see
this message (with your username of course): “I: Logged in as ‘joshgavant’ on
‘https://api.openshift.com’.</p>

<p><img src="../assets/openshift_on_aws/console-downloads-token.png" width="300px" />
<img src="../assets/openshift_on_aws/ocm-manage-token.png" width="300px" /></p>

<blockquote>
  <p>Tip: To quickly enable autocompletion for <code class="highlighter-rouge">rosa</code> commands in your current
  shell session run <code class="highlighter-rouge">. &lt;(rosa completion)</code>.</p>
</blockquote>

<p>To verify that you’ve logged in successfully to both accounts run <code class="highlighter-rouge">rosa whoami</code>.</p>

<h3 id="iam-roles">IAM Roles</h3>

<p>Next you’ll need to create an AWS IAM role specifying the permissions Red Hat’s
cluster manager service and operations team members will require. In STS mode,
these roles will be applied to the short-lived tokens issued to these operators
on demand. Run the following commands to create the OCM service role and the
operator user role:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rosa create <span class="nt">--yes</span> ocm-role <span class="nt">--admin</span> <span class="nt">--mode</span><span class="o">=</span>auto <span class="nt">--prefix</span><span class="o">=</span><span class="s2">"ManagedOpenShift"</span>
rosa create <span class="nt">--yes</span> user-role <span class="nt">--mode</span><span class="o">=</span>auto <span class="nt">--prefix</span><span class="o">=</span><span class="s2">"ManagedOpenShift"</span>
rosa create <span class="nt">--yes</span> account-roles <span class="nt">--mode</span><span class="o">=</span>auto <span class="nt">--prefix</span><span class="o">=</span><span class="s2">"ManagedOpenShift"</span>
</code></pre></div></div>

<h3 id="create-cluster">Create cluster</h3>

<p>Now that you’ve bound your Red Hat account with your AWS account you can proceed
to create your ROSA cluster! If you’d like to use the rosa CLI, run the
following command to create a cluster in <a href="https://docs.openshift.com/rosa/rosa_getting_started/rosa-sts-getting-started-workflow.html">STS
mode</a>.</p>

<p>Note that by setting the <code class="highlighter-rouge">--watch</code> flag installation logs will stream to stdout
and the command won’t return till installation completes successfully or fails,
typically &gt;30 minutes. You can not set set that flag, or exit the log stream
with Ctrl+c, then watch logs again with <code class="highlighter-rouge">rosa logs install --cluster ${CLUSTER_NAME} --watch</code>.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">## create a ROSA cluster in STS mode</span>
rosa create <span class="nt">--yes</span> cluster <span class="nt">--cluster-name</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--sts</span> <span class="nt">--mode</span><span class="o">=</span>auto <span class="nt">--watch</span>
</code></pre></div></div>

<h3 id="monitor-installation">Monitor installation</h3>

<p>Note that by setting the <code class="highlighter-rouge">--watch</code> flag installation logs will stream to stdout
and the command won’t return till installation completes successfully or fails,
typically &gt;30 minutes. You can not set set that flag, or exit the log stream
with Ctrl+c, then watch logs again with <code class="highlighter-rouge">rosa logs install --cluster ${CLUSTER_NAME} --watch</code>.</p>

<p>You can also see your new cluster in the <a href="https://console.redhat.com/openshift">Red Hat
Console</a>. Click into it and expand the
“Show logs” section to reach a view like the following:</p>

<p><img src="../assets/openshift_on_aws/view-cluster-webui.png" width="300px" /></p>

<h3 id="use-cluster">Use cluster</h3>

<p>Once it’s ready, the easiest way to begin using your cluster immediately is to
create a one-off <code class="highlighter-rouge">cluster-admin</code> user as follows. Later you can allow users from
a specific OpenIDConnect (OIDC) identity provider.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">## create a cluster-admin user</span>
rosa create <span class="nt">--yes</span> admin <span class="nt">--cluster</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">"</span>
</code></pre></div></div>

<p>Next, you’ll need URLs to reach the API server and Console of your new cluster!
Get those with <code class="highlighter-rouge">rosa list clusters</code>. Finally, log in to the cluster via the <code class="highlighter-rouge">oc</code>
CLI: <code class="highlighter-rouge">oc login --user cluster-admin --password ${admin_password}</code>.</p>

<h3 id="create-cluster-via-ui">Create cluster via UI</h3>

<p>Note that once your Red Hat and AWS accounts are linked you can also choose to
create a cluster via a guided wizard in the Console. On the <a href="https://console.redhat.com/openshift">Clusters
page</a> on the Red Hat Console click “Create
cluster”, then on the <a href="https://console.redhat.com/openshift/create">Cluster create
page</a> click “Create cluster” next
to the ROSA offering, as in the following screenshot:</p>

<p><img src="../assets/openshift_on_aws/create-cluster-webui.png" width="300px" /></p>

<p>If you’ve properly associated your accounts then your AWS account will be listed
(by its ID) on the first page of the wizard. Follow the prompts to configure and
install a cluster.</p>

<h2 id="installer-provisioned-infrastructure-ipi">Installer-provisioned infrastructure (IPI)</h2>

<p>Follow along with <a href="https://github.com/joshgav/openshift-on-aws/tree/main/ipi">the scripts</a>.</p>

<p>Even if your cluster won’t be managed by Red Hat you can provision and configure
cloud infrastructure and the cluster itself in AWS with a similar short list of
commands. Red Hat calls this installation method “Installer-provisioned
infrastructure” (IPI). Here’s how to do it.</p>

<h3 id="setup-1">Setup</h3>

<p>The core of the IPI method is the <strong><code class="highlighter-rouge">openshift-install</code></strong> CLI; download it from
the <a href="https://console.redhat.com/openshift/downloads#tool-x86_64-openshift-install">downloads section</a>
of the OpenShift console, or directly from
<a href="https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/stable/openshift-install-linux.tar.gz">https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/stable/openshift-install-linux.tar.gz</a>.</p>

<p>You’ll also need a <strong>pull secret</strong> containing credentials for Red Hat’s
container registries. Copy this from
<a href="https://console.redhat.com/openshift/downloads#tool-pull-secret">https://console.redhat.com/openshift/downloads#tool-pull-secret</a>.</p>

<p>A <strong>SSH key pair</strong> is required for access to provisioned machines; you’ll need
to provide its public key to the installer and you’ll be able to use the private
key for access. You can copy an existing key from (for example)
<code class="highlighter-rouge">~/.ssh/id_rsa.pub</code>; or create a new one in a secure place using (for example)
<code class="highlighter-rouge">ssh-keygen -t rsa -b 4096 -C "user@openshift" -f "${WORKDIR}/id_rsa" -N ''</code>.
Copy the contents of the <code class="highlighter-rouge">*.pub</code> file as the value of <code class="highlighter-rouge">SSH_PUBLIC_KEY</code> below.</p>

<p>Finally, you’ll need an AWS <strong>Route53 public hosted zone</strong> for your cluster’s
base domain name. For example, I delegate a domain named <code class="highlighter-rouge">aws.joshgav.com</code> from
my registrar to a new AWS Route53 zone, see following screenshot. Specifically,
after creating the Route53 zone I create NS records for <code class="highlighter-rouge">aws</code> in the parent
<code class="highlighter-rouge">joshgav.com</code> zone pointing to the name servers selected by Route53. More
details <a href="https://docs.openshift.com/container-platform/4.10/installing/installing_aws/installing-aws-account.html#installation-aws-route53_installing-aws-account">from RedHat
here</a>
and <a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/CreatingNewSubdomain.html">from AWS
here</a>.</p>

<p><img src="../assets/openshift_on_aws/aws-route53.png" width="300px" /></p>

<h3 id="create-cluster-1">Create cluster</h3>

<p>Now you can use <code class="highlighter-rouge">openshift-install create ...</code> to independently manage various
phases of the installation process. A simple, automatable approach is to define
the desired state of your cluster in a file named <code class="highlighter-rouge">install-config.yaml</code> put it
in a directory <code class="highlighter-rouge">${WORKDIR}</code> and run the installer with it as follows:
<code class="highlighter-rouge">openshift-install create cluster --dir ${WORKDIR}</code>. Following is a template
install-config.yaml file; use it with your own values for
<code class="highlighter-rouge">OPENSHIFT_PULL_SECRET</code>, <code class="highlighter-rouge">YOUR_DOMAIN_NAME</code> and <code class="highlighter-rouge">SSH_PUBLIC_KEY</code> established
above.</p>

<blockquote>
  <p>NOTE: The schema for install-config is in <a href="https://github.com/openshift/installer/blob/master/pkg/types/installconfig.go">https://github.com/openshift/installer/blob/master/pkg/types/installconfig.go</a>.</p>
</blockquote>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">ipi</span>
<span class="na">baseDomain</span><span class="pi">:</span> <span class="s">${YOUR_DOMAIN_NAME}</span>
<span class="na">controlPlane</span><span class="pi">:</span>
  <span class="na">architecture</span><span class="pi">:</span> <span class="s">amd64</span>
  <span class="na">hyperthreading</span><span class="pi">:</span> <span class="s">Enabled</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">master</span>
  <span class="na">platform</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
<span class="na">compute</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">architecture</span><span class="pi">:</span> <span class="s">amd64</span>
  <span class="na">hyperthreading</span><span class="pi">:</span> <span class="s">Enabled</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">worker</span>
  <span class="na">platform</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
<span class="na">networking</span><span class="pi">:</span>
  <span class="na">networkType</span><span class="pi">:</span> <span class="s">OVNKubernetes</span>
  <span class="na">clusterNetwork</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">cidr</span><span class="pi">:</span> <span class="s">10.128.0.0/14</span>
    <span class="na">hostPrefix</span><span class="pi">:</span> <span class="m">23</span>
  <span class="na">machineNetwork</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">cidr</span><span class="pi">:</span> <span class="s">10.0.0.0/16</span>
  <span class="na">serviceNetwork</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">172.30.0.0/16</span>
<span class="na">platform</span><span class="pi">:</span>
  <span class="na">aws</span><span class="pi">:</span>
    <span class="na">region</span><span class="pi">:</span> <span class="s">us-east-1</span>
<span class="na">publish</span><span class="pi">:</span> <span class="s">External</span>
<span class="na">pullSecret</span><span class="pi">:</span> <span class="s1">'</span><span class="s">${OPENSHIFT_PULL_SECRET}'</span>
<span class="na">sshKey</span><span class="pi">:</span> <span class="s1">'</span><span class="s">${SSH_PUBLIC_KEY}'</span>
</code></pre></div></div>

<h3 id="monitor-installation-1">Monitor installation</h3>

<p>Cluster installation will take 30 minutes or more. You can watch logs stream to
stdout or tail the .openshift_install.log file in the installation working
directory.</p>

<h3 id="use-cluster-1">Use cluster</h3>

<p>A username and password for your cluster will be in the final lines of the log,
either on stdout or in the <code class="highlighter-rouge">.openshift_install.log</code> file. In addition, a
“kubeconfig” file and the kubeadmin user’s password are saved in the <code class="highlighter-rouge">auth</code>
directory of the installation dir. Login to your cluster with one of the
following:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">## using kubeconfig with embedded certificate</span>
<span class="nb">export </span><span class="nv">KUBECONFIG</span><span class="o">=</span>temp/_workdir/auth/kubeconfig

<span class="c">## using username and password</span>
oc login <span class="nt">--user</span> kubeadmin <span class="nt">--password</span> <span class="s2">"</span><span class="si">$(</span><span class="nb">cat </span>temp/_workdir/auth/kubeadmin-password<span class="si">)</span><span class="s2">"</span>

<span class="c">## verify authorization</span>
oc get pods <span class="nt">-A</span>
</code></pre></div></div>

<h2 id="user-provisioned-infrastructure-upi">User-provisioned infrastructure (UPI)</h2>

<p>Follow along with <a href="https://github.com/joshgav/openshift-on-aws/tree/main/upi">the scripts</a>.</p>

<p>Though the easiest way to get started with OpenShift on AWS is via ROSA or
installer-provisioned infrastructure (IPI), Red Hat also allows you to deploy
and configure your own cloud infrastructure - machines, networks and storage -
and provision a cluster over it. This is known as “user-provisioned
infrastructure” - UPI.</p>

<p>UPI installations still use the <code class="highlighter-rouge">openshift-install</code> CLI to generate resource
manifests and Ignition configuration files. However it is up to the user to
configure machines and supply these Ignition files to them. In AWS this is
accomplished by putting a configuration in a S3 bucket and pointing the first
machine there at startup.</p>

<p>Red Hat provides a set of CloudFormation templates reflecting good patterns for
provisioning supporting infrastructure for an OpenShift cluster; these templates
are available
<a href="https://github.com/openshift/installer/tree/master/upi/aws/cloudformation">here</a>.
In the <code class="highlighter-rouge">upi</code> directory in the repo which accomplanies this article a <code class="highlighter-rouge">deploy.sh</code>
script steps through the following:</p>

<ol>
  <li>Create manifests and configurations with <code class="highlighter-rouge">openshift-install</code></li>
  <li>Deploy AWS networks and machines using recommended CloudFormation templates</li>
  <li>Await completed installation using <code class="highlighter-rouge">openshift-install</code></li>
</ol>

<p>The AWS resources created for OpenShift include a VPC and subnets, a DNS zone
and records, load balancers and target groups, IAM roles, security groups and
even an S3 bucket. They also include several machine types - bootstrap, control
plane and worker. The bootstrap machine is provisioned first and installs the
production cluster on the other machines.</p>

<p>Full instructions for AWS UPI are <a href="https://docs.openshift.com/container-platform/4.10/installing/installing_aws/installing-aws-user-infra.html">here</a>.</p>

<p>Note that one step in the process can be difficult to automate - signing
Certificate Signing Requests (CSRs) for nodes. Check if CSRs are awaiting a
signature with <code class="highlighter-rouge">oc get csr</code>. Approve all pending requests with the following:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">csrs</span><span class="o">=(</span><span class="si">$(</span>oc get csr <span class="nt">-o</span> json | jq <span class="nt">-r</span> <span class="s1">'.items[] | select(.status == {}) | .metadata.name'</span><span class="si">)</span><span class="o">)</span>
<span class="k">for </span>csr <span class="k">in</span> <span class="s2">"</span><span class="k">${</span><span class="nv">csrs</span><span class="p">[@]</span><span class="k">}</span><span class="s2">"</span><span class="p">;</span> <span class="k">do
    </span>oc adm certificate approve <span class="s2">"</span><span class="k">${</span><span class="nv">csr</span><span class="k">}</span><span class="s2">"</span>
<span class="k">done</span>
</code></pre></div></div>

<p>As with IPI, you can monitor the <code class="highlighter-rouge">.openshift_install.log</code> file for progress of
cluster installation. When the cluster is ready, log in with <code class="highlighter-rouge">oc login</code> as the
kubeadmin user with the password in <code class="highlighter-rouge">${workdir}/auth/kubeadmin-password</code>, or set
your KUBECONFIG env var to the path <code class="highlighter-rouge">${workdir}/auth/kubeconfig</code>.</p>

<p>Once ready reach the console of your cluster at
<code class="highlighter-rouge">https://console-openshift-console.apps.${CLUSTER_NAME}.${BASE_DOMAIN}/</code>.</p>

<h2 id="kubespray">Kubespray</h2>

<p>Follow along with <a href="https://github.com/joshgav/openshift-on-aws/tree/main/kubespray">the scripts</a>.</p>

<p>The previous sections described how to deploy OpenShift, Red Hat’s Kubernetes
distribution, on Amazon Web Services with various levels of support and
automation. Next we’ll deploy upstream Kubernetes using
<a href="https://kubespray.io">Kubespray</a> in order to compare, contrast and gather new
ideas. Notably, kubespray’s included configuration for AWS infrastructure yields
an environment nearly identical to that produced by openshift-install.</p>

<blockquote>
  <p>Note: The most basic cluster installation tool is <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">kubeadm</a>, but it leaves
many critical aspects of the cluster incomplete, such as a network overlay,
container registry and load balancer controller. <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubespray/">Kubespray</a> is also
maintained by the Kubernetes project and provides a more complete deployment.</p>
</blockquote>

<p>As with user-provisioned infrastructure (UPI) for OpenShift, with Kubespray the
user must first install infrastructure, then use Kubespray to install a cluster
on that infrastructure. Kubespray offers Terraform configurations for deploying
typical environments in cloud providers. For this example I used the
<a href="https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/aws">configurations for
AWS</a>
which yields the following env:</p>

<p><img src="../assets/openshift_on_aws/aws-kubespray.png" width="300px" /></p>

<blockquote>
  <p>From <a href="https://github.com/kubernetes-sigs/kubespray/blob/master/contrib/terraform/aws/docs/aws_kubespray.png">https://github.com/kubernetes-sigs/kubespray/blob/master/contrib/terraform/aws/docs/aws_kubespray.png</a></p>
</blockquote>

<p>The infrastructure provisioning process finishes by creating an inventory file
which Ansible will consume to deploy cluster components. Now you’ll run the main
Kubespray process - an Ansible playbook - using that inventory:
<code class="highlighter-rouge">ansible-playbook -i hosts.ini cluster.yaml</code>. You can customize the deployment
by setting variables in the inventory vars files or by passing <code class="highlighter-rouge">-e key=value</code>
pairs to the ansible-playbook invocation. See <code class="highlighter-rouge">deploy-cluster.sh</code> in the
walkthrough for examples. So that you don’t have to install the Kubespray
Ansible environment locally, you may prefer to run commands like the following
in a container:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>podman run <span class="nt">--rm</span> <span class="nt">-it</span> <span class="se">\</span>
    <span class="nt">--mount</span> <span class="nb">type</span><span class="o">=</span><span class="nb">bind</span>,source<span class="o">=</span>kubespray/inventory/cluster,dst<span class="o">=</span>/inventory,relabel<span class="o">=</span>shared <span class="se">\</span>
    <span class="nt">--mount</span> <span class="nb">type</span><span class="o">=</span><span class="nb">bind</span>,source<span class="o">=</span>.ssh/id_rsa,dst<span class="o">=</span>/root/.ssh/id_rsa,relabel<span class="o">=</span>shared <span class="se">\</span>
        quay.io/kubespray/kubespray:v2.19.0 <span class="se">\</span>
            bash

<span class="c"># when prompted, enter (for example):</span>
ansible-playbook cluster.yml <span class="se">\</span>
    <span class="nt">-i</span> /inventory/hosts.ini <span class="se">\</span>
    <span class="nt">--private-key</span> /root/.ssh/id_rsa <span class="se">\</span>
    <span class="nt">--become</span> <span class="nt">--become-user</span><span class="o">=</span>root <span class="se">\</span>
    <span class="nt">-e</span> <span class="s2">"kube_version=v1.23.7"</span> <span class="se">\</span>
    <span class="nt">-e</span> <span class="s2">"ansible_user=ec2-user"</span> <span class="se">\</span>
    <span class="nt">-e</span> <span class="s2">"kubeconfig_localhost=true"</span>
</code></pre></div></div>

<p>By setting the variable <code class="highlighter-rouge">kubeconfig_localhost=true</code> a kubeconfig file with
credentials for the provisioned cluster will be written to the inventory
directory at the end of provisioning. It will use the internal IP address of an
API server; you’ll need to change this to the address of your
externally-accessible load balancer. Retrieve that with <code class="highlighter-rouge">aws elbv2
describe-load-balancers --output json | jq -r '.LoadBalancers[0].DNSName'</code>, and
be sure to prepend <code class="highlighter-rouge">https://</code> and append <code class="highlighter-rouge">:6443/</code> when putting it in the file.
Finally set your KUBECONFIG env var to point to that file and run <code class="highlighter-rouge">kubectl get
pods -A</code> to verify connectivity.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this article and accompanying code we’ve discussed and demonstrated how to
deploy an OpenShift or upstream Kubernetes cluster in AWS using four different
methods which progress from simplest to most complex: ROSA &gt; IPI &gt; UPI &gt;
Kubespray.</p>

<p>To minimize the complexity and overhead of managing your own clouds and
clusters, start with the simplest method - ROSA - and progress to others only as
greater control and customization is needed.</p>

<p>Please provide feedback in <a href="https://github.com/joshgav/openshift-on-aws/">the
repo</a> or on
<a href="https://twitter.com/joshugav">Twitter</a>. Thank you!</p>]]></content><author><name>Josh Gavant</name></author><category term="clusters" /><category term="openshift" /><summary type="html"><![CDATA[OpenShift is a Kubernetes distribution with batteries included - it’s ready for many use cases out of the box. For example, a default installation includes a network provider (CNI), container builder and registry, external ingress provider and a cluster lifecycle management system. Contrast this with a cluster provisioned by kubeadm or even the more featureful kubespray, where adding these and other critical features requires thoughtful and often fragile integration.]]></summary></entry><entry><title type="html">Clusters for all!</title><link href="https://blog.joshgav.com//cluster-level-multitenancy" rel="alternate" type="text/html" title="Clusters for all!" /><published>2022-05-16T16:00:00+00:00</published><updated>2022-05-16T16:00:00+00:00</updated><id>https://blog.joshgav.com//cluster-level-multitenancy</id><content type="html" xml:base="https://blog.joshgav.com//cluster-level-multitenancy"><![CDATA[<p><img src="/assets/virtual_cluster/pleiades.jpg" alt="Pleiades star clusters" /></p>

<p>A decision which faces many large organizations as they adopt cloud architecture is how to provide isolated spaces within the same environments and clusters for various teams and purposes. For example, marketing and sales applications may need to be isolated from an organization’s customer-facing applications; and development teams building any app usually require extra spaces for tests and verification.</p>

<h2 id="namespace-as-unit-of-tenancy">Namespace as unit of tenancy</h2>

<p>To address this need, many organizations have started to use namespaces as units of isolation and tenancy, a pattern previously described by <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/multitenancy-overview">Google</a> and <a href="https://kubernetes.io/blog/2021/04/15/three-tenancy-models-for-kubernetes/">Kubernetes contributors</a>. But namespace-scoped isolation is often insufficient because some concerns are managed at cluster scope. In particular, installing new resource types (CRDs) is a cluster-scoped activity; and today independent teams often want to install custom resource types and operators. Also, more developers are themselves writing software operators and custom resource types and find themselves requiring cluster-scoped access for research and tests.</p>

<h2 id="cluster-as-unit-of-tenancy">Cluster as unit of tenancy</h2>

<p>For these reasons and others, tenants often require their own isolated clusters with unconstrained access rights. In an isolated cluster, a tenant gets its own Kubernetes API server and persistence store and fully manages all namespaces and custom resource types in its cluster.</p>

<p>But deploying physical or even virtual machines for many clusters is inefficient and difficult to manage, so organizations have struggled to provide clusters to tenant teams. Happily :smile:, to meet these organizations’ and users’ needs, leading Kubernetes vendors have been researching and developing lighter weight mechanisms to provide isolated clusters for an organization’s tenants. In this post we’ll compare and contrast several of these emergent efforts.</p>

<p>Do you have other projects and ideas to enhance multitenancy for cloud architecture? Then please join CNCF’s App Delivery advisory group in discussing these <a href="https://github.com/cncf/tag-app-delivery/issues/193">here</a>; thank you!</p>

<h3 id="vcluster">vcluster</h3>

<p><a href="https://www.vcluster.com/">vcluster</a> is <a href="https://www.google.com/search?q=vcluster&amp;tbm=nws">a prominent project</a> and CLI tool maintained by <a href="https://loft.sh/">loft.sh</a> that provisions a virtual cluster as a StatefulSet within a tenant namespace. Access rights from the hosting namespace are propogated to the hosted virtual cluster such that the namespace tenant becomes the cluster’s only tenant. As cluster admins, tenant members can create cluster-scoped resources like CRDs and ClusterRoles.</p>

<p>The virtual cluster runs its own Kubernetes API service and persistence store independent of those of the hosting cluster. It can be published by the hosting cluster as a LoadBalancer-type service and accessed directly with kubectl and other Kubernetes API-compliant tools. This enables users of the tenant cluster to work with it directly with little or no knowledge of its host.</p>

<p><img src="/assets/virtual_cluster/vcluster.png" alt="vcluster architecture" /></p>

<p>A high-level perspective of vcluster’s architecture.</p>

<p>In vcluster and the following solutions, the virtual cluster is a “metadata-only” cluster, in that resources in it are persisted to a backing store like etcd, but no schedulers act to reify the persisted resources - ultimately as pods. Instead, a “syncer” synchronization service copies and transforms reifiable resources - podspecs - from the virtual cluster to the hosting namespace of the hosting cluster. Schedulers in the hosting cluster then detect and reify these resources in the same underlying tenant namespace where the virtual cluster’s control plane runs.</p>

<p>An advantage of vcluster’s approach of scheduling pods in the hosting namespace is that the hosting cluster ultimately handles all workloads and applies namespace quotas - all work happens within the namespace allocated to the tenant by the hosting cluster administrator. A disadvantage is that schedulers cannot be configured in the virtual cluster since pods aren’t actually run there. (Update: vcluster now supports a virtual scheduler, see <a href="https://www.vcluster.com/docs/architecture/scheduling#separate-vcluster-scheduler">https://www.vcluster.com/docs/architecture/scheduling#separate-vcluster-scheduler</a>.)</p>

<ul>
  <li><a href="https://github.com/loft-sh/vcluster">vcluster on GitHub</a></li>
</ul>

<h3 id="cluster-api-provider-nested-capn">Cluster API Provider Nested (CAPN)</h3>

<p>In vcluster, bespoke support for control plane implementations is required; as of this writing, vcluster supports k3s, k0s and vanilla k8s distributions.</p>

<p>To support <em>any</em> control plane implementation, the <a href="https://github.com/kubernetes-sigs/cluster-api-provider-nested">Cluster API Provider Nested</a> project implements an architecture similar to that of vcluster, including a metadata-only cluster and a syncer, but provisions the control plane using a Cluster API provider rather than a bespoke distribution.</p>

<p><img src="/assets/virtual_cluster/capn.png" alt="capn architecture" /></p>

<p>A high-level perspective of CAPN’s architecture.</p>

<p>CAPN promises to enable control planes implementable via Cluster API to serve virtual clusters.</p>

<h3 id="hypershift">HyperShift</h3>

<p>Similar to the previous two, <a href="https://www.redhat.com/">Red Hat</a>’s <a href="https://github.com/openshift/hypershift">HyperShift</a> project provisions an OpenShift (Red Hat’s Kubernetes distro) control plane as a collection of pods in a host namespace. But rather than running workloads within the hosting cluster and namespace like vcluster, HyperShift control planes are connected to a pool of dedicated worker nodes where pods are synced and scheduled.</p>

<p><img src="/assets/virtual_cluster/hypershift.png" alt="HyperShift architecture" /></p>

<p>A high-level perspective of HyperShift’s architecture.</p>

<p>HyperShift’s model may be most appropriate for a hosting provider like Red Hat which desires to abstract control plane management from their customers and allow them to just manage worker nodes.</p>

<h3 id="kcp">kcp</h3>

<p>Finally, <a href="https://github.com/kcp-dev/kcp">kcp</a> is another proposal and project from <a href="https://www.redhat.com/">Red Hat</a> inspired by and reimagined from all of the previous ideas. Whereas the above virtual clusters run <em>within</em> a host cluster and turn to the host cluster to run pods, manage networks and provision volumes, kcp reverses this paradigm and makes the <em>hosting</em> cluster a metadata-only cluster. <em>Child</em> clusters - <em>workspaces</em> in the kcp project - are registered with the hub metadata-only cluster and work is delegated to these children based on labels on resources in the hub.</p>

<p><img src="/assets/virtual_cluster/kcp.png" alt="kcp architecture" /></p>

<p>A high-level perspective of kcp’s architecture.</p>

<p>As opposed to hosted virtual clusters, child clusters in kcp <em>could</em> manage their own schedulers. Another advantage of kcp’s paradigm inversion is centralized awareness and management of child clusters. In particular, this enables simpler centralized policies and standards for custom resource types to be propogated to all children.</p>

<h2 id="conclusion">Conclusion</h2>

<p>vcluster, CAPN, HyperShift, and kcp are emerging projects and ideas to meet cloud users’ needs for multitenancy with <em>clusters</em> as the unit of tenancy. Early adopters are already providing feedback on good and better parts of these approaches and new ideas emerge daily.</p>

<p>Want to help drive new ideas for cloud multitenancy? Want to help cloud users understand and give feedback on emerging paradigms in this domain? Then join <a href="https://github.com/cncf/tag-app-delivery/issues/193">the discussion</a> in CNCF’s TAG App Delivery. Thank you!</p>

<h4 id="colophon">Colophon</h4>

<p>The image at the top is of star clusters in <a href="https://en.wikipedia.org/wiki/Pleiades">Pleiades</a> and the picture was copied from that article.</p>]]></content><author><name>Josh Gavant</name></author><category term="multitenancy" /><category term="clusters" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Kubernetes isn’t about containers</title><link href="https://blog.joshgav.com//kubernetes-isnt-about-containers" rel="alternate" type="text/html" title="Kubernetes isn’t about containers" /><published>2021-12-16T19:00:00+00:00</published><updated>2021-12-16T19:00:00+00:00</updated><id>https://blog.joshgav.com//kubernetes-isnt-about-containers</id><content type="html" xml:base="https://blog.joshgav.com//kubernetes-isnt-about-containers"><![CDATA[<p>It’s about APIs; we’ll get to that shortly.</p>

<h2 id="first-there-were-containers">First there were containers</h2>

<p>Now Docker <em>is</em> about containers: running complex software with a simple <code class="highlighter-rouge">docker run postgres</code> command was a revelation to software developers in 2013, unlocking agile infrastructure that they’d never known. And happily, as developers adopted containers as a standard build and run target, the industry realized that the same encapsulation fits nicely for workloads to be scheduled in compute clusters by orchestrators like Kubernetes and Apache Mesos. Containers have become the most important workload type managed by these schedulers, but as the title says that’s not what’s most valuable about Kubernetes.</p>

<p>Kubernetes is not about more general workload <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/">scheduling</a> either (sorry <a href="https://krustlet.dev/">Krustlet</a> fans). While scheduling various workloads efficiently is an important value Kubernetes provides, it’s not the reason for its success.</p>

<h2 id="then-there-were-apis">Then there were APIs</h2>

<p><img width="400" alt="Always has been APIs" src="/assets/always_has_been_apis.jpeg" /></p>

<p>Rather, the attribute of Kubernetes that’s made it so successful and valuable is that <strong>it provides a set of standard programming interfaces for writing and using software-defined infrastructure services</strong>. Kubernetes provides specifications and implementations - a complete framework - for designing, implementing, operating and using infrastructure services of all shapes and sizes based on the same core structures and semantics: typed resources watched and reconciled by controllers.</p>

<p>To elaborate, consider what preceded Kubernetes: a hodge-podge of hosted “cloud” services with different APIs, descriptor formats, and semantic patterns. We’d piece together compute instances, block storage, virtual networks and object stores in one cloud; and in another we’d create the same using entirely different structures and APIs. Tools like Terraform came along and offered a common format across providers, but the original structures and semantics remained as variegated as ever - a Terraform descriptor targeting AWS stands no chance in Azure!</p>

<p>Now consider what Kubernetes provided from its earliest releases: standard APIs for describing compute requirements as pods and containers; virtual networking as services and eventually ingresses; persistent storage as volumes; and even workload identities as attestable service accounts. These formats and APIs work smoothly within Kubernetes distributions running everywhere, from public clouds to private datacenters. Internally, each provider maps the Kubernetes structures and semantics to that hodge-podge of native APIs mentioned in the previous paragraph.</p>

<p>Kubernetes offers a standard interface for managing software-defined infrastructure - <a href="https://joshgav.github.io/2021/09/30/cloud-redefined-infrastructure.html">cloud</a>, in other words. <strong>Kubernetes is a standard API framework for cloud services.</strong></p>

<h2 id="and-then-there-were-more-apis">And then there were more APIs</h2>

<p>Providing a fixed set of standard structures and semantics is the foundation of Kubernetes’ success. Following on this, its next act is to extend that structure to <em>any and all</em> infrastructure resources. <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resource Definitions</a> (CRDs) were introduced in version 1.7 to allow other types of services to reuse Kubernetes’ programming framework. CRDs make it possible to request not only predefined compute, storage and network services from the Kubernetes API, but also databases, task runners, message buses, digital certificates, and whatever else a provider can imagine!</p>

<p>As providers have sought to offer their services via the Kubernetes API as custom resources, the <a href="https://operatorframework.io/">Operator Framework</a> and related projects from <a href="https://github.com/kubernetes/community/tree/master/sig-api-machinery">SIG API Machinery</a> have emerged to provide tools and guidance that minimize work required and maximize standardization across all these shiny new resource types. Projects like <a href="https://crossplane.io">Crossplane</a> have formed to map other provider resources like RDS databases and SQS queues into the Kubernetes API just like network interfaces and disks are handled by core Kubernetes controllers today. And Kubernetes distributors like <a href="https://cloud.google.com/blog/topics/developers-practitioners/build-platform-krm-part-2-how-kubernetes-resource-model-works">Google</a> and <a href="https://docs.openshift.com/container-platform/4.9/operators/understanding/crds/crd-managing-resources-from-crds.html">Red Hat</a> are providing more and more custom resource types in their base Kubernetes distributions.</p>

<p>All of this isn’t to say that the Kubernetes API framework is perfect. Rather it’s to say that <em>it doesn’t matter</em> (much) because the Kubernetes model has become a de facto standard. Many developers understand it, many tools speak it, and many providers use it. Even with warts, Kubernetes’ broad adoption, user awareness and interoperability mostly outweigh other considerations.</p>

<p>With the spread of the Kubernetes resource model it’s already possible to describe an entire software-defined computing environment as a collection of Kubernetes resources. Like running a single artifact with <code class="highlighter-rouge">docker run ...</code>, distributed applications can be deployed and run with a simple <code class="highlighter-rouge">kubectl apply -f ...</code>. And unlike the custom formats and tools offered by individual cloud service providers, the Kubernetes’ descriptors are much more likely to run in many different provider and datacenter environments, because <strong>they all implement the same APIs</strong>.</p>

<p>Kubernetes isn’t about containers after all. It’s about APIs.</p>]]></content><author><name>Josh Gavant</name></author><category term="containers" /><category term="kubernetes" /><category term="apis" /><category term="infrastructure" /><summary type="html"><![CDATA[It’s about APIs; we’ll get to that shortly.]]></summary></entry><entry><title type="html">Cloud redefines enterprise infrastructure</title><link href="https://blog.joshgav.com//cloud-redefined-infrastructure" rel="alternate" type="text/html" title="Cloud redefines enterprise infrastructure" /><published>2021-09-30T13:00:00+00:00</published><updated>2021-09-30T13:00:00+00:00</updated><id>https://blog.joshgav.com//cloud-redefined-infrastructure</id><content type="html" xml:base="https://blog.joshgav.com//cloud-redefined-infrastructure"><![CDATA[<p><img src="/assets/infra_desired_state.png" alt="infrastructure platform" /></p>

<p>Cloud computing has been around for well over a decade, so we ought to know what “cloud” is by now. Indeed we understand its attributes well, such as flexibility, efficiency, connectivity, and scalability; in the <a href="https://github.com/cncf/toc/blob/main/DEFINITION.md">words of the CNCF</a> (emphasis added):</p>

<blockquote>
  <p>Cloud native technologies empower organizations to build and run <strong>scalable</strong> applications in modern, <strong>dynamic</strong> environments…  These techniques enable loosely coupled systems that are <strong>resilient</strong>, <strong>manageable</strong>, and <strong>observable</strong>… They allow engineers to make high-impact changes <strong>frequently</strong> and predictably with minimal toil.</p>
</blockquote>

<p>But what is “cloud” itself, provider of all these desirable attributes? Particularly in our era of hybrid, multi, and private “clouds” we should clearly define the term. And how does “cloud” relate to the rest of our infrastructure?</p>

<h2 id="what-is-cloud">What is cloud?</h2>

<p>Let us describe “cloud” by induction from existing ones: a <strong>cloud</strong> is a collection of automatable <em>infrastructure services</em> managed via a consistent set of interfaces - APIs, Web UIs, and CLIs. An <strong>infrastructure service</strong> is a service which serves other services which in turn serve users - an infrastructure service serves end users only indirectly. Stated a bit differently, <strong>an infrastructure service serves applications</strong> and their developers and <strong>a cloud is a collection of such services</strong>.</p>

<p>Per this definition, any provider of infrastructure services might be considered a cloud provider, though generally we reserve the title for providers that offer many diverse service types. “Multi-cloud” environments are those using infrastructure services from several providers. A “private cloud” is a collection of infrastructure services offered via an internal, perhaps custom interface.</p>

<p>With this in mind let’s now describe how cloud is changing enterprise infrastructure. The graphic above illustrates these.</p>

<h3 id="from-servers-to-serverless">From servers to serverless</h3>

<p>Before “cloud” many infrastructure specialists managed hardware servers, datacenters, network devices and operating system configurations. But “cloud” is replacing most such physical components with programmable, software-defined ones - virtual machines, virtual networks, dynamic datastores and queues, and much more. The job of infrastructure specialists is now to <strong>program virtual infrastructure</strong>.</p>

<p>A consistent, thorough, app-centric interface is one reasonable ultimate realization of cloud as defined, so “serverless” is a good example of a cloud interface for infrastructure driven strictly by app requirements. The most common paradigm though for programming virtual infrastructure today is to virtualize and automate existing architectural patterns - such as describing a router, a pod, and a datastore as software-defined Kubernetes resources and having a controller reify them.</p>

<p>Whatever the interface though, <strong>infrastructure is not about hardware anymore, it’s about software</strong>.</p>

<h3 id="from-services-to-platform">From services to platform</h3>

<p>Another change is that before “cloud” infrastructure services were often offered by different teams via different interfaces. An identity or TLS certificate was issued by one team; another would provision a collector for logs and metrics; and another would deploy servers, networks and operating systems. Each would collect metadata and implement requirements from app teams in their own ways.</p>

<p>But as infrastructure services become software-defined, interfaces and processes for acquiring those services can become more consistent, easier and faster for apps and developers to use and manage. For example, a set of Kubernetes resources or Terraform manifests could describe every infrastructure service required by an application.</p>

<p>In other words, cloud is an opportunity to bring together a bunch of disparate services and interfaces into a consistent <strong>platform</strong>.</p>

<h3 id="from-dependency-to-partner">From dependency to partner</h3>

<p>As infrastructure becomes more flexible and more capable of quickly fulfilling app developers’ needs, infrastructure teams are enabled to partner and agilely develop services together with application developers. The agility enabled by software allows infrastructure teams to deliver their set of services as a cohesive product to app developers, gathering and iterating quickly on feedback and new requirements.</p>

<p>With cloud, <strong>infrastructure becomes an active partner to product teams in delivering business value</strong> to customers and reacting to new circumstances.</p>

<h2 id="conclusion">Conclusion</h2>

<p>A “cloud” is <em>not</em> just something run by big tech companies like Microsoft and Amazon. Rather, “cloud” is <em>the</em> new paradigm of enterprise infrastructure itself: providing a consistent collection of automatable infrastructure services to apps and developers.</p>

<p>How are you providing “cloud” at your organization?</p>]]></content><author><name>Josh Gavant</name></author><category term="platform" /><category term="infrastructure" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Deliver platform capabilities</title><link href="https://blog.joshgav.com//deliver-platform-capabilities" rel="alternate" type="text/html" title="Deliver platform capabilities" /><published>2021-08-30T13:00:00+00:00</published><updated>2021-08-30T13:00:00+00:00</updated><id>https://blog.joshgav.com//deliver-platform-capabilities</id><content type="html" xml:base="https://blog.joshgav.com//deliver-platform-capabilities"><![CDATA[<p><img src="/assets/city-platform.jpg" alt="Platform and capabilities" /></p>

<p>Platform teams exist to develop and manage capabilities required across many application service teams. When functioning well, platform teams provide expertise and support for complex capabilities which would otherwise have required such support in each application service team.</p>

<p>This post describes how a platform team may develop and manage these <strong>platform capabilities</strong> and gradually evolve a capabilities development and delivery <em>framework</em>. The framework should not be designed up front, but platform architects should help it emerge by building prototypical application services and developing initial capabilities for them, as described herein.</p>

<h2 id="develop-prototype-application-services">Develop prototype application services</h2>

<p>A prerequisite for developing a platform-level shared capability is a representative prototype of the application services wherein the capability will be used. For example, to research and develop service-to-service authentication and authorization mechanisms, two communicating application-level services must exist first; to develop a traffic management capability, one must have application services to route traffic to.</p>

<p>And so we come to the first step in developing platform capabilities: <strong>platform developers and application developers must cooperate to develop prototype application services that are truly representative of application-level scenarios</strong>. Not only must the two groups develop <em>initial</em> prototypes, they must also continuously improve and expand such prototypes to cover more scenarios and adapt to inevitable changes in the organization’s environment.</p>

<p>Architects and product managers should do the following to develop prototype application services:</p>

<ul>
  <li>Interview leads, reverse-engineer codebases and conduct experiments to identify and prioritize the most important and most typical scenarios and required capabilities for the organization’s application services</li>
  <li>Ensure all prototype application service code is parameterized and names are extracted</li>
  <li>Ensure that any senior developer can automatically reify a development environment with a single command; and conduct research and development using the prototypes</li>
</ul>

<p>Several iterations with several application service teams and platform capability teams will probably be required to refine prototype code and ensure coverage of essential scenarios.</p>

<h2 id="develop-and-deliver-capabilities">Develop and deliver capabilities</h2>

<p>As development of prototype application services begins, development of initial individual capabilities may also begin, as well as early planning for a standard capability development and delivery framework. Early capability development should focus on individual capabilities rather than a general framework; experiments using these initial individual capabilities will guide and inform planning and design of the greater framework. Just as prototype application services guide development of individual capabilities, so too individual capabilities guide development of a capability framework.</p>

<h3 id="develop-capabilities">Develop capabilities</h3>

<p>Capabilities should be developed and released as follows:</p>

<ol>
  <li>Define or refine definition of desired capability</li>
  <li>Research and develop implementations for capability</li>
  <li>Deliver capability</li>
  <li>Gather feedback and learnings and go to 1</li>
</ol>

<p>For example, a capability for managing secret configuration might follow this storyline:</p>

<ol>
  <li>Define desired capability:
    <ul>
      <li>inject secret configuration as key-value pairs at service start time</li>
    </ul>
  </li>
  <li>Research and develop implementations for capability
    <ul>
      <li>inject Vault agent configuration as pod sidecar using Helm charts</li>
      <li>deploy Vault admission controller system</li>
      <li>integrate Vault secrets with Kubernetes Secrets using <a href="https://secrets-store-csi-driver.sigs.k8s.io/">Secrets Store CSI Driver</a></li>
    </ul>
  </li>
  <li>Deliver capability
    <ul>
      <li>document how to configure Vault agent in a pod sidecar</li>
      <li>automatically configure Vault agent with default configuration via a Helm chart</li>
      <li>automatically configure Vault agent via an admission controller</li>
    </ul>
  </li>
  <li>Gather feedback and learnings and iterate
    <ul>
      <li>adjust exposed configuration options and APIs</li>
      <li>change implementation</li>
      <li>support TLS secrets specially</li>
    </ul>
  </li>
</ol>

<p>Every capability team will require its members to have expert-level domain knowledge in that capability, alleviating the need for developers in application teams to be expert in the domain. Developers on the team will develop the internal implementation of the capability based on their expertise in it, application service requirements, and the constraints imposed by the organization’s environment. PMs will share that knowledge with other partners and users. Team members will participate in external industry communities related to their domain as well to keep abreast of new opportunities and developments.</p>

<p>Capabilities themselves may be implemented in many ways. They have often been built as programming language libraries to be imported into applications in source code, at build time and/or at run time. In cloud-native applications, capabilities are often built as independent processes which communicate with a service’s main process via transports like HTTP, GRPC or TCP. In Kubernetes, supporting processes are typically deployed as sidecars in a pod or daemonsets on every node. Mutating admission controllers can also modify capabilities of a pod by modifying their configuration.</p>

<p>As capabilities are developed, a development framework should also emerge to assist with and coordinate fruitful patterns and should include the following:</p>

<ul>
  <li>Several viable options and examples for developing platform capabilities within the organization, such as operators, templating tools, daemonsets or pod sidecars</li>
  <li>Ability to provision a productive development environment for app and platform research and development</li>
</ul>

<h3 id="deliver-capabilities">Deliver capabilities</h3>

<p>When ready, platform teams must package and deliver their capability to be used by application teams. To this end, platform architects must gradually develop and manage a standard framework to guide capability developers in delivering and supporting their capabilities, and to provide a rational, consistent experience for application developers using them.</p>

<p>Once a given capability has been initially developed, it should be integrated and tested in application services in the following progression, which a framework should emerge to guide and facilitate:</p>

<ol>
  <li>Document required capability configuration and inform users how to set up the capability manually
    <ol>
      <li>Verify that application developers are satisfied with the <em>functionality</em> of the capability</li>
    </ol>
  </li>
  <li>Inject capability and default configuration via toggles and tags
    <ol>
      <li>Verify that application developers are satisfied with the exposed configuration options</li>
    </ol>
  </li>
  <li>Inject capability automatically and transparently
    <ol>
      <li>At build time with e.g., <code class="highlighter-rouge">helm</code>, <code class="highlighter-rouge">kustomize</code></li>
      <li>At deploy time with e.g., mutating admission controllers</li>
      <li>At run time with e.g., daemonsets and network proxies</li>
    </ol>
  </li>
</ol>

<p>Just as <em>application</em> delivery is only complete when customers begin using the application, platform <em>capability</em> delivery is only complete when application teams begin using the capability in production. The top goal of a platform capability framework should be to provide guided, standard ways to deliver platform capabilities to application developers. As individual capabilities are developed and tested, good general designs and strategies for capability delivery and integration will emerge and should influence development and evolution of the general framework.</p>

<p>Dimensions a platform capability delivery framework should consider making possible include the following:</p>

<ul>
  <li>To enable the capability, must the capability team team a) deploy and manage a service such as an operator; or b) package and publish a library; or c) something else?</li>
  <li>To enable a capability, must application developers a) explicitly integrate the capability in source code or infrastructure configuration or b) is it transparently injected?</li>
  <li>May application developers a) toggle and configure a capability or b) not?</li>
</ul>

<h2 id="summary">Summary</h2>

<p>Platform teams promise to multiply the efficiency of application service teams by centralizing knowledge about and management of shared application capabilities like identity and observability. Help both platform and application teams succeed by providing platform capability development and delivery frameworks to guide them.</p>]]></content><author><name>Josh Gavant</name></author><category term="platform" /><category term="infrastructure" /><category term="architecture" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">How container streaming (exec, port-forward) works in Kubernetes</title><link href="https://blog.joshgav.com//container-streaming" rel="alternate" type="text/html" title="How container streaming (exec, port-forward) works in Kubernetes" /><published>2020-09-16T16:30:00+00:00</published><updated>2020-09-16T16:30:00+00:00</updated><id>https://blog.joshgav.com//container-streaming</id><content type="html" xml:base="https://blog.joshgav.com//container-streaming"><![CDATA[<h1 id="overview-of-kubelet">Overview of Kubelet</h1>

<p>Kubernetes’ <strong>kubelet</strong> is a server and controller which runs on every cluster node as an agent to allocate compute, storage and network resources for workloads described by <strong>PodSpec</strong>s retrieved from the API server. Not only does the kubelet manage pods for Kubernetes in “connected” mode, but it can also (or alternatively) read PodSpecs from the local filesystem or an HTTP endpoint in “standalone” mode. In short, the kubelet is an independent implementation of PodSpec.</p>

<blockquote>
  <p>For more on kubelet’s standalone mode check out <a href="https://coreos.com/blog/introducing-the-kubelet-in-coreos.html">this article</a> and <a href="https://github.com/kelseyhightower/standalone-kubelet-tutorial">this tutorial</a> from Kelsey Hightower.</p>
</blockquote>

<p>Kubelet handles the heavy lifting of provisioning virtual networks, allocating and attaching block storage and running container images by calling a <strong>container runtime</strong> like Docker via Kubelet’s Container Runtime Interface (CRI), as defined in <a href="https://github.com/kubernetes/cri-api/blob/master/pkg/apis/runtime/v1alpha2/api.proto">this protobuf spec</a>. A shim for CRI from Docker’s native API is <a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/kubelet/dockershim">included</a> in the kubelet, or you can follow <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">these instructions</a> to install and use another runtime like CRI-O.</p>

<p>Amongst the procedures offered by CRI’s <a href="https://github.com/kubernetes/cri-api/blob/205a053b09eb766d86191392b3e6bd94df6ceb0c/pkg/apis/runtime/v1alpha2/api.proto#L33-L110">RuntimeService</a> one finds <strong>Exec</strong>, <strong>Attach</strong> and <strong>PortForward</strong>, likely familiar to anyone who works with containers. These ultimately are core to the <code class="highlighter-rouge">kubectl exec ...</code>, <code class="highlighter-rouge">kubectl run -it ...</code>, <code class="highlighter-rouge">kubectl port-forward</code> and even the new <code class="highlighter-rouge">kubectl [alpha] debug ...</code> commands that container developers know and love. Following is how these commands and procedures work together to connect your terminal to a process in a worker node.</p>

<h1 id="exec">exec</h1>

<p>First let’s walk through what happens when you run <code class="highlighter-rouge">kubectl exec -it ${pod_name} sh --container ${container_name}</code> to run a shell in the context of an existing container. We’ll borrow and refer to the following diagram from <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/20191205-container-streaming-requests.md">this k8s enhancement proposal</a>.</p>

<p><img src="https://raw.githubusercontent.com/kubernetes/enhancements/master/keps/sig-node/1558-streaming-proxy-redirects/kubelet-proxied-streaming-request-sequence.png" style="margin-left: 40px;" /></p>

<h2 id="1-client">1. client</h2>

<p>Based on its arguments, <code class="highlighter-rouge">kubectl exec</code> builds a URL for and opens an HTTP/2 connection with the API server. The local terminal’s standard I/O streams (stdin, stdout, stderr) are connected to this transport. The URL formed for the API server is <code class="highlighter-rouge">http[s]://${api_server}/ns/${pod_namespace}/pods/${pod_name}/exec?stdin=true&amp;stdout=true&amp;stderr=true&amp;tty=true&amp;container=${container_name}&amp;command=sh</code>.</p>

<h3 id="source-refs">Source refs:</h3>

<ul>
  <li><code class="highlighter-rouge">kubectl exec</code> [<a href="https://github.com/kubernetes/kubectl/blob/d70ead5fcaa0e8f8246715584147ba3bfd081411/pkg/cmd/exec/exec.go">1</a>]</li>
</ul>

<h2 id="2-apiserver">2. apiserver</h2>

<p>The corev1/pods APIService accepts the incoming request and handles it per <a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/registry/core/pod">its registration</a>. Specifically, it discovers the address and port of the Node/Kubelet running the indicated container and opens a streaming proxy connection to it. This stream is bound to the streams from the incoming request.</p>

<p>The URL for the kubelet server is of form <code class="highlighter-rouge">http[s]://${node_ip}:${kubelet_port}/${subresource}/${pod_namespace}/${pod_name}/${container_name}</code> where <code class="highlighter-rouge">${subresource}</code> can be <code class="highlighter-rouge">exec</code>, <code class="highlighter-rouge">attach</code>, <code class="highlighter-rouge">portforward</code> or a few others.</p>

<h3 id="source-refs-1">Source refs:</h3>

<ul>
  <li><code class="highlighter-rouge">registry/core/pod.streamLocation</code> [<a href="https://github.com/kubernetes/kubernetes/blob/9621ac6ec7eddccdf007c043272c81b23408704b/pkg/registry/core/pod/strategy.go#L506-L511">1</a>]</li>
</ul>

<h2 id="3-kubelet">3. kubelet</h2>

<p>The kubelet provides its own API server which accepts the incoming request from the API server and forwards it to the container runtime. The kubelet then continues to proxy I/O streams between the API server and the container runtime. An option exists to hand off the stream with the container runtime directly to the API server (rather than continuing to proxy it through kubelet), but it has been deprecated.</p>

<p>The exec, attach, port-forward and logs actions are handled by Kubelet’s “debugging handlers.” They can be disabled by setting <a href="https://github.com/kubernetes/kubelet/blob/f87179761b5b3b817cf86fdf2e31801c61a8db7e/config/v1beta1/types.go#L255-L262">EnableDebuggingHandlers</a> to <code class="highlighter-rouge">false</code> in the global kubelet configuration, or by setting the flag <code class="highlighter-rouge">--enable-debugging-handlers=false</code> on an individual kubelet. <strong>Note</strong> that this will disable container logs via <code class="highlighter-rouge">kubectl logs</code> as well!</p>

<h3 id="source-refs-2">Source refs:</h3>

<ul>
  <li><code class="highlighter-rouge">kubelet/server/NewServer(enableDebuggingHandlers)</code> [<a href="https://github.com/kubernetes/kubernetes/blob/3d52b8b5d60e1f74f4207f1d046734878297e354/pkg/kubelet/server/server.go#L243-L253">1</a>]</li>
  <li><code class="highlighter-rouge">kubelet/server/server.InstallDebuggingHandlers</code> [<a href="https://github.com/kubernetes/kubernetes/blob/3d52b8b5d60e1f74f4207f1d046734878297e354/pkg/kubelet/server/server.go#L411">2</a>]</li>
  <li><code class="highlighter-rouge">kubelet/server/server.getExec</code> [<a href="https://github.com/kubernetes/kubernetes/blob/3d52b8b5d60e1f74f4207f1d046734878297e354/pkg/kubelet/server/server.go#L795-L821">3</a>]</li>
</ul>

<h2 id="4-cri">4. CRI</h2>

<p>Finally the container runtime - or runtime shim in Docker’s case - receives the request from kubelet and takes the steps necessary to create and execute a process in the namespaces and cgroups of the target container. In Docker this is achieved by calling <a href="https://pkg.go.dev/github.com/moby/moby/client#Client.ContainerExecCreate">github.com/moby/moby/client#Client.ContainerExecCreate</a>.</p>

<p>In truth <code class="highlighter-rouge">exec</code> itself can be executed without a persistent connection, in which case you wouldn’t be able to send stdin or receive stdout from the executed command. When you specify <code class="highlighter-rouge">-i -t</code> with <code class="highlighter-rouge">exec</code> an attach action is executed immediately after exec to provide a persistent connection.</p>

<h3 id="source-refs-3">Source refs:</h3>

<ul>
  <li><code class="highlighter-rouge">kubelet/cri/streaming.NewServer</code> [<a href="https://github.com/kubernetes/kubernetes/blob/e83412c331ae72718a84623870c420e6daf58a25/pkg/kubelet/cri/streaming/server.go#L125-L133">1</a>]</li>
  <li><code class="highlighter-rouge">kubelet/cri/streaming/server.serveExec</code> [<a href="https://github.com/kubernetes/kubernetes/blob/e83412c331ae72718a84623870c420e6daf58a25/pkg/kubelet/cri/streaming/server.go#L265-L297">2</a>]</li>
  <li><code class="highlighter-rouge">kubelet/cri/streaming/remotecommand/ServeExec</code> [<a href="https://github.com/kubernetes/kubernetes/blob/e83412c331ae72718a84623870c420e6daf58a25/pkg/kubelet/cri/streaming/remotecommand/exec.go#L44">3</a>]</li>
  <li><code class="highlighter-rouge">kubelet/dockershim/NativeExecHandler.ExecInContainer</code> [<a href="https://github.com/kubernetes/kubernetes/blob/fe1aeff2d2341e3d9a553534c814ad40f8219e35/pkg/kubelet/dockershim/exec.go#L64">4</a>]</li>
  <li><code class="highlighter-rouge">kubelet/dockershim/libdocker/kubeDockerClient.StartExec</code> [<a href="https://github.com/kubernetes/kubernetes/blob/e83412c331ae72718a84623870c420e6daf58a25/pkg/kubelet/dockershim/libdocker/kube_docker_client.go#L461">5</a>]</li>
  <li><code class="highlighter-rouge">moby/moby/client/Client.ContainerExecCreate</code> [<a href="https://pkg.go.dev/github.com/moby/moby/client#Client.ContainerExecCreate">6</a>]</li>
</ul>

<h1 id="port-forward">port-forward</h1>

<p>Whereas exec and attach work in the context of a container, port-forward communicates with the “pod”, or more specifically with the pod’s network namespace. In Kubelet’s built-in Docker CRI shim, port forwarding is accomplished with the following command. The “sandbox” in CRI represents the pod context.</p>

<p><code class="highlighter-rouge">nsenter -t ${sandbox_pid} -n socat - TCP4:localhost:${target_port}</code></p>

<h3 id="source-refs-4">Source Refs</h3>

<ul>
  <li><code class="highlighter-rouge">kubelet/cri/streaming/portforward.ServePortForward</code> [<a href="https://github.com/kubernetes/kubernetes/blob/e83412c331ae72718a84623870c420e6daf58a25/pkg/kubelet/cri/streaming/portforward/portforward.go#L36-L53">1</a>]</li>
  <li><code class="highlighter-rouge">kubelete/dockershim/streamingRuntime.portForward</code> [<a href="https://github.com/kubernetes/kubernetes/blob/e83412c331ae72718a84623870c420e6daf58a25/pkg/kubelet/dockershim/docker_streaming_others.go">2</a>]</li>
</ul>

<h1 id="logs">logs</h1>

<p>Requests for container logs also pass through the kubelet to the CRI and are streamed back to the client.</p>

<h3 id="source-refs-5">Source Refs</h3>

<ul>
  <li><code class="highlighter-rouge">kubelet/server/server.InstallDebuggingHandlers</code> [<a href="https://github.com/kubernetes/kubernetes/blob/fd9828b02a786d4fa8d2add04c37e33a616d0087/pkg/kubelet/server/server.go#L482-L488">1</a>]</li>
  <li><code class="highlighter-rouge">kubelet/server/server.getContainerLogs</code> [<a href="https://github.com/kubernetes/kubernetes/blob/fd9828b02a786d4fa8d2add04c37e33a616d0087/pkg/kubelet/server/server.go#L595-L661">2</a>]</li>
  <li><code class="highlighter-rouge">dockershim/dockerService.GetContainerLogs</code> [<a href="https://github.com/kubernetes/kubernetes/blob/fe1aeff2d2341e3d9a553534c814ad40f8219e35/pkg/kubelet/dockershim/docker_legacy_service.go#L49-L92">3</a>]</li>
</ul>]]></content><author><name>Josh Gavant</name></author><category term="kubernetes" /><category term="kubelet" /><category term="exec" /><summary type="html"><![CDATA[Overview of Kubelet]]></summary></entry><entry><title type="html">Introducing Partly Cloudy</title><link href="https://blog.joshgav.com//welcome-to-partly-cloudy" rel="alternate" type="text/html" title="Introducing Partly Cloudy" /><published>2020-04-17T16:21:49+00:00</published><updated>2020-04-17T16:21:49+00:00</updated><id>https://blog.joshgav.com//welcome-to-partly-cloudy</id><content type="html" xml:base="https://blog.joshgav.com//welcome-to-partly-cloudy"><![CDATA[<p>So called because I’ll be writing posts about cloud-native patterns, practices and tools.</p>

<p>I chose to start by maintaining the site with <a href="https://jekyllrb.com/">Jekyll</a>, though I almost reconsidered upon discovering that the latest version of Jekyll <a href="https://github.com/github/pages-gem/issues/651">isn’t fully supported</a> for automatically-built GitHub Pages sites. That led me to write <a href="https://github.com/joshgav/joshgav.github.io/blob/master/.github/workflows/publish-site.yaml">a GitHub Action to build and publish the site</a>, inspired by <a href="https://sujaykundu.com/blog/post/deploy-jekyll-using-github-pages-and-github-actions">this post</a>. Feeling confident with that in hand, I pushed the first version and was pleasantly surprised to find the GitHub Pages <em>did</em> automatically build the site. Good thing too cause my custom Action originally pushed the built site to the <code class="highlighter-rouge">gh-pages</code> branch, which wouldn’t work for a user’s top-level user site - <code class="highlighter-rouge">joshgav.github.io</code> in my case.</p>

<p>I wasn’t happy with the unpolished feel of the default <code class="highlighter-rouge">minima</code> theme, so I reviewed some others at <a href="https://jekyllthemes.io/">jekyllthemes.io</a>. That site offers Jekyll themes for $$ too; I’m not sure of its business model. In any case it helped me find a simple theme named “<a href="https://github.com/huangyz0918/moving">moving</a>” which I liked. I installed it by editing my config files, testing locally, and pushing the changes to GitHub. But… my site didn’t render and I received an email notification that GitHub Pages only works with <a href="https://pages.github.com/themes/">this subset of Jekyll themes</a>. Great, an opportunity to tweak and use the Action action!</p>

<p>So to use my chosen theme I went back and tweaked my action to listen for pushes to the <code class="highlighter-rouge">source</code> branch and push changes to the <code class="highlighter-rouge">master</code> branch, as required for the “top-level” GitHub Page. It seems to be working now.</p>

<p>Oh, if you’re wondering I chose Jekyll cause it’s an “elder statesman” of static site generation by now and has a community of users and plugin developers. That it’s the official SSG for GitHub Pages also lent it favor.</p>]]></content><author><name>Josh Gavant</name></author><category term="info" /><category term="blog" /><summary type="html"><![CDATA[So called because I’ll be writing posts about cloud-native patterns, practices and tools.]]></summary></entry></feed>